{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(torch.nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate intersection and union for each sample in the batch\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        intersection = torch.sum(inputs * targets, dim=(1, 2, 3))  # Sum over height, width, and channels\n",
    "        total = torch.sum(inputs, dim=(1, 2, 3)) + torch.sum(targets, dim=(1, 2, 3))\n",
    "        \n",
    "        # Calculate Dice coefficient with epsilon for numerical stability\n",
    "        dice_coefficient = (2. * intersection + self.epsilon) / (total + self.epsilon)\n",
    "        \n",
    "        # Calculate Dice Loss for each sample in the batch\n",
    "        dice_loss = 1 - dice_coefficient\n",
    "        \n",
    "        # Calculate the average loss over the batch\n",
    "        dice_loss = torch.mean(dice_loss)\n",
    "        \n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegData(Dataset):\n",
    "\n",
    "    def __init__(self,image_paths,mask_paths,img_mean,img_std,transform = None):\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.img_mean = img_mean\n",
    "        self.img_std = img_std\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[index]).convert('L')\n",
    "\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "\n",
    "        image = torch.from_numpy(image).to(torch.float32)\n",
    "        mask = torch.from_numpy(mask).to(torch.long)\n",
    "\n",
    "        image = torch.unsqueeze(image,0)\n",
    "        mask = torch.unsqueeze(torch.unsqueeze(mask,dim=0),dim=0)\n",
    "        \n",
    "        basic_image_transform = transforms.Compose(transforms=[\n",
    "            transforms.Resize(size = (256,256)),\n",
    "            transforms.Normalize(mean = self.img_mean,std = self.img_std),\n",
    "        ])\n",
    "\n",
    "        basic_mask_transform = transforms.Compose(transforms=[\n",
    "            transforms.Resize(size = (256,256)),\n",
    "            transforms.Lambda(lambda x: x/torch.max(x)),\n",
    "        ])\n",
    "\n",
    "        image = basic_image_transform(image)\n",
    "        mask = basic_mask_transform(mask)\n",
    "\n",
    "        image = image.squeeze()\n",
    "        mask = mask.squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        sample = {'image':image,'mask':mask}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/UFAD/s.kapoor/GrainGrowth/skapoor/dl_mis/COVID-19_Radiography_Dataset/COVID-19_Radiography_Dataset'\n",
    "data_fold = [x.name for x in os.scandir(data_dir) if x.is_dir()]\n",
    "data_fold.remove('.ipynb_checkpoints')\n",
    "\n",
    "total_input_files = []\n",
    "total_output_files = []\n",
    "for fold in data_fold:\n",
    "    fold_dir = os.path.join(data_dir,fold)\n",
    "    for subfold in [x.name for x in os.scandir(os.path.join(data_dir,fold)) if (x.is_dir and x.name[0]!='.' and x.name!='Thumbs.db')]:\n",
    "        subfold_dir = os.path.join(fold_dir,subfold)\n",
    "        if subfold == 'images':\n",
    "            total_input_files.extend([os.path.join(subfold_dir,img_name) for img_name in os.listdir(os.path.join(fold_dir,subfold)) if img_name!='Thumbs.db'])\n",
    "        elif subfold == 'masks':\n",
    "            total_output_files.extend([os.path.join(subfold_dir,img_name) for img_name in os.listdir(os.path.join(fold_dir,subfold)) if img_name!='Thumbs.db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_img,w_img = Image.open(total_input_files[0]).convert('RGB').size\n",
    "# #get normalization values for the complete dataset\n",
    "# red_sum = 0\n",
    "# # blue_sum = 0\n",
    "# # green_sum = 0\n",
    "# # because orignal data is grey scale so values are distributed evenly across all three channels.\n",
    "# for files in total_input_files:\n",
    "\n",
    "#     pil_image = Image.open(files).convert('RGB')\n",
    "#     np_image = np.array(pil_image)\n",
    "\n",
    "#     red_values = np_image[:,:,0].flatten()\n",
    "#     # green_values = np_image[:,:,1].flatten()\n",
    "#     # blue_values = np_image[:,:,2].flatten()\n",
    "\n",
    "#     red_sum += np.sum(red_values)\n",
    "#     # blue_sum += np.sum(blue_values)\n",
    "#     # green_sum += np.sum(green_values)\n",
    "\n",
    "#     del(red_values)\n",
    "#     # del(green_values)\n",
    "#     # del(blue_values)\n",
    "\n",
    "# total_mean = [red_sum/(h_img*w_img*len(total_input_files))]*3\n",
    "\n",
    "# red_std_sum = 0\n",
    "# for files in total_input_files:\n",
    "\n",
    "#     pil_image = Image.open(files).convert('RGB')\n",
    "#     np_image = np.array(pil_image)\n",
    "\n",
    "#     red_values = (np_image[:,:,0].flatten() - total_mean[0])**2\n",
    "#     red_std_sum += np.sum(red_values)\n",
    "\n",
    "#     del(red_values)\n",
    "\n",
    "# total_std = [math.sqrt(red_std_sum/(h_img*w_img*len(total_input_files)))]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    seed = 13\n",
    "    torch.manual_seed(seed)\n",
    "    best_v_loss = 1_000_000\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    model = Unet(3,1)\n",
    "    model.to(device)\n",
    "    epochs = 10\n",
    "    loss_func = SoftDiceLoss()\n",
    "    loss_func.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer,factor = 0.8,min_lr=1e-6)\n",
    "    pth = os.getcwd()\n",
    "    model_path = os.path.join(pth,'model_3_soft_dice_loss')\n",
    "    log_dir = os.path.join(model_path,'runs')\n",
    "    wts_path = os.path.join(model_path,'weigths')\n",
    "\n",
    "    if os.path.exists(model_path) == False:\n",
    "        os.mkdir(model_path)\n",
    "        \n",
    "        os.mkdir(log_dir)\n",
    "        \n",
    "        os.mkdir(wts_path)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    #splitting the dataset into traininig and validation set\n",
    "    train_img,valid_img,train_mask,valid_mask = train_test_split(total_input_files,total_output_files,test_size=0.2,random_state=seed)\n",
    "    train_data = SegData(image_paths = train_img,mask_paths=train_mask,img_mean=[129.908, 129.908, 129.908],img_std=[64.484, 64.484, 64.484])\n",
    "    valid_data = SegData(image_paths = valid_img,mask_paths=valid_mask,img_mean=[129.908, 129.908, 129.908],img_std=[64.484, 64.484, 64.484])\n",
    "    train_dataloader = DataLoader(train_data,batch_size = 4,shuffle = True,pin_memory = True)\n",
    "    valid_dataloader = DataLoader(valid_data,batch_size = 4,shuffle = False,pin_memory = True)\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    #print(\"After Model to device : \",torch.cuda.memory_allocated(device)/(1024*1024))\n",
    "    for e in tqdm(range(epochs)):\n",
    "\n",
    "        #training the model \n",
    "        print('Epoch {0}'.format(e+1))\n",
    "        epoch_loss = 0.0\n",
    "        run_loss = 0.0\n",
    "        model.train(True)\n",
    "\n",
    "        for i,batch in enumerate(train_dataloader):\n",
    "            \n",
    "            image = batch['image'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "    \n",
    "            #print(batch['image'].shape)\n",
    "            v_img = transforms.RandomVerticalFlip(p=1)(image)\n",
    "            h_img = transforms.RandomHorizontalFlip(p=1)(image)\n",
    "            r_img = transforms.RandomRotation(degrees=(-25,25))(image)\n",
    "            image_final = torch.cat((image,v_img,h_img,r_img),0)\n",
    "\n",
    "            #print(image_final.shape)\n",
    "                    \n",
    "            v_mask = transforms.RandomVerticalFlip(p=1)(mask)\n",
    "            h_mask = transforms.RandomHorizontalFlip(p=1)(mask)\n",
    "            r_mask = transforms.RandomRotation(degrees=(-25,25))(mask)\n",
    "            mask_final = torch.cat((mask,v_mask,h_mask,r_mask),0)\n",
    "            mask_final = torch.unsqueeze(mask_final,1)\n",
    "            #print(mask_final.shape)\n",
    "            \n",
    "#             image_final.to(device)\n",
    "#             mask_final.to(device)\n",
    "            \n",
    "#             print(image_final.dtype)\n",
    "#             print(mask_final.dtype)\n",
    "#             print(device)\n",
    "            #a = torch.cuda.memory_allocated(device)/(1024*1024)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model.forward(image_final)\n",
    "            #print(\"after forward pass\", torch.cuda.memory_allocated(device)/(1024*1024))\n",
    "            #b = torch.cuda.memory_allocated(device)/(1024*1024)\n",
    "            #print(\"memory consumed by forward_pass\",b-a)\n",
    "            loss = loss_func(pred,mask_final)\n",
    "            \n",
    "\n",
    "            run_loss = run_loss + loss.item()\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            #print(\"memory after backward pass \", torch.cuda.memory_allocated(device)/(1024*1024))\n",
    "            optimizer.step()\n",
    "            #print(\"memory after optimizer \",torch.cuda.memory_allocated(device)/(1024*1024))\n",
    "\n",
    "            #print('the current loss is {0}'.format(run_loss))\n",
    "            \n",
    "#             del mask_final\n",
    "#             del image_final\n",
    "#             del pred\n",
    "#             del v_img\n",
    "#             del h_img\n",
    "#             del r_img\n",
    "#             del v_mask\n",
    "#             del h_mask\n",
    "#             del r_mask\n",
    "#             del mask\n",
    "#             del image\n",
    "            \n",
    "            #print(\"memory after delete \",torch.cuda.memory_allocated(device)/(1024*1024))\n",
    "            \n",
    "\n",
    "            if(i%100 == 99):\n",
    "                \n",
    "#                 for obj in gc.get_objects():\n",
    "#                     try:\n",
    "#                         if torch.is_tensor(obj) or (hasattr(obj,'data') and torch.is_tensor(obg.data)):\n",
    "#                             print(type(obj),obj.size())\n",
    "#                     except:\n",
    "#                         pass\n",
    "                \n",
    "\n",
    "                print(\"\\tBatch Loss for curent for {0} is {1:.5f}\".format(i,run_loss/100))\n",
    "                run_loss = 0.0\n",
    "                \n",
    "\n",
    "        avg_e_loss = epoch_loss/(i+1)\n",
    "        print('The average loss for the epoch is {0}'.format(avg_e_loss))\n",
    "\n",
    "        #validating the model\n",
    "        model.train(False)\n",
    "\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for k,batch in enumerate(valid_dataloader):\n",
    "            final_image = batch['image'].to(device)\n",
    "            final_mask = torch.unsqueeze(batch['mask'],1).to(device)\n",
    "\n",
    "            pred = model.forward(final_image)\n",
    "            loss = loss_func(pred,final_mask)\n",
    "\n",
    "            val_loss+= loss.item()\n",
    "            \n",
    "#             del final_image\n",
    "#             del final_mask\n",
    "#             break\n",
    "        \n",
    "        \n",
    "\n",
    "        avg_val_loss = val_loss/(k+1)\n",
    "        print('The average validation loss for the epoch is {0:.5f}'.format(avg_val_loss))\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_e_loss, 'Validation' : avg_val_loss },\n",
    "                    e)\n",
    "        counter+=1\n",
    "        if avg_val_loss < best_v_loss:\n",
    "\n",
    "            best_v_loss = avg_val_loss\n",
    "            best_wt_path = os.path.join(wts_path,'best_epoch'+str(e))\n",
    "            counter = 0\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "\n",
    "        #break\n",
    "        wt_path =  os.path.join(wts_path,'epoch_'+str(e))\n",
    "        torch.save(model.state_dict(), wt_path)\n",
    "        \n",
    "        if counter>= patience:\n",
    "            print('Early stopping after epoch {0}'.format(e))\n",
    "            \n",
    "    torch.save(model.state_dict(), best_wt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tBatch Loss for curent for 99 is 0.39036\n",
      "\tBatch Loss for curent for 199 is 0.20598\n",
      "\tBatch Loss for curent for 299 is 0.18085\n",
      "\tBatch Loss for curent for 399 is 0.15112\n",
      "\tBatch Loss for curent for 499 is 0.15544\n",
      "\tBatch Loss for curent for 599 is 0.13714\n",
      "\tBatch Loss for curent for 699 is 0.12014\n",
      "\tBatch Loss for curent for 799 is 0.10695\n",
      "\tBatch Loss for curent for 899 is 0.10940\n",
      "\tBatch Loss for curent for 999 is 0.11419\n",
      "\tBatch Loss for curent for 1099 is 0.10566\n",
      "\tBatch Loss for curent for 1199 is 0.09809\n",
      "\tBatch Loss for curent for 1299 is 0.10199\n",
      "\tBatch Loss for curent for 1399 is 0.10633\n",
      "\tBatch Loss for curent for 1499 is 0.09999\n",
      "\tBatch Loss for curent for 1599 is 0.09869\n",
      "\tBatch Loss for curent for 1699 is 0.08940\n",
      "\tBatch Loss for curent for 1799 is 0.08937\n",
      "\tBatch Loss for curent for 1899 is 0.08942\n",
      "\tBatch Loss for curent for 1999 is 0.09195\n",
      "\tBatch Loss for curent for 2099 is 0.08894\n",
      "\tBatch Loss for curent for 2199 is 0.08693\n",
      "\tBatch Loss for curent for 2299 is 0.09454\n",
      "\tBatch Loss for curent for 2399 is 0.09138\n",
      "\tBatch Loss for curent for 2499 is 0.08316\n",
      "\tBatch Loss for curent for 2599 is 0.08394\n",
      "\tBatch Loss for curent for 2699 is 0.08616\n",
      "\tBatch Loss for curent for 2799 is 0.09109\n",
      "\tBatch Loss for curent for 2899 is 0.08995\n",
      "\tBatch Loss for curent for 2999 is 0.08833\n",
      "\tBatch Loss for curent for 3099 is 0.08458\n",
      "\tBatch Loss for curent for 3199 is 0.08584\n",
      "\tBatch Loss for curent for 3299 is 0.08424\n",
      "\tBatch Loss for curent for 3399 is 0.08645\n",
      "\tBatch Loss for curent for 3499 is 0.08488\n",
      "\tBatch Loss for curent for 3599 is 0.08270\n",
      "\tBatch Loss for curent for 3699 is 0.07362\n",
      "\tBatch Loss for curent for 3799 is 0.07940\n",
      "\tBatch Loss for curent for 3899 is 0.08243\n",
      "\tBatch Loss for curent for 3999 is 0.08092\n",
      "\tBatch Loss for curent for 4099 is 0.08217\n",
      "\tBatch Loss for curent for 4199 is 0.08084\n",
      "The average loss for the epoch is 0.1071768479319208\n",
      "The average validation loss for the epoch is 0.03149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [30:42<4:36:25, 1842.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "\tBatch Loss for curent for 99 is 0.07471\n",
      "\tBatch Loss for curent for 199 is 0.07384\n",
      "\tBatch Loss for curent for 299 is 0.07844\n",
      "\tBatch Loss for curent for 399 is 0.07402\n",
      "\tBatch Loss for curent for 499 is 0.07718\n",
      "\tBatch Loss for curent for 599 is 0.07889\n",
      "\tBatch Loss for curent for 699 is 0.07923\n",
      "\tBatch Loss for curent for 799 is 0.07189\n",
      "\tBatch Loss for curent for 899 is 0.08009\n",
      "\tBatch Loss for curent for 999 is 0.08009\n",
      "\tBatch Loss for curent for 1099 is 0.07651\n",
      "\tBatch Loss for curent for 1199 is 0.07226\n",
      "\tBatch Loss for curent for 1299 is 0.07527\n",
      "\tBatch Loss for curent for 1399 is 0.08019\n",
      "\tBatch Loss for curent for 1499 is 0.08018\n",
      "\tBatch Loss for curent for 1599 is 0.07881\n",
      "\tBatch Loss for curent for 1699 is 0.06979\n",
      "\tBatch Loss for curent for 1799 is 0.07366\n",
      "\tBatch Loss for curent for 1899 is 0.07318\n",
      "\tBatch Loss for curent for 1999 is 0.07335\n",
      "\tBatch Loss for curent for 2099 is 0.07443\n",
      "\tBatch Loss for curent for 2199 is 0.07086\n",
      "\tBatch Loss for curent for 2299 is 0.07426\n",
      "\tBatch Loss for curent for 2399 is 0.07186\n",
      "\tBatch Loss for curent for 2499 is 0.07041\n",
      "\tBatch Loss for curent for 2599 is 0.07374\n",
      "\tBatch Loss for curent for 2699 is 0.06891\n",
      "\tBatch Loss for curent for 2799 is 0.07890\n",
      "\tBatch Loss for curent for 2899 is 0.07118\n",
      "\tBatch Loss for curent for 2999 is 0.07477\n",
      "\tBatch Loss for curent for 3099 is 0.06936\n",
      "\tBatch Loss for curent for 3199 is 0.06889\n",
      "\tBatch Loss for curent for 3299 is 0.07337\n",
      "\tBatch Loss for curent for 3399 is 0.07205\n",
      "\tBatch Loss for curent for 3499 is 0.07244\n",
      "\tBatch Loss for curent for 3599 is 0.07493\n",
      "\tBatch Loss for curent for 3699 is 0.07722\n",
      "\tBatch Loss for curent for 3799 is 0.07007\n",
      "\tBatch Loss for curent for 3899 is 0.07692\n",
      "\tBatch Loss for curent for 3999 is 0.07616\n",
      "\tBatch Loss for curent for 4099 is 0.07011\n",
      "\tBatch Loss for curent for 4199 is 0.06965\n",
      "The average loss for the epoch is 0.0743601790356729\n",
      "The average validation loss for the epoch is 0.02667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [1:01:22<4:05:28, 1841.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "\tBatch Loss for curent for 99 is 0.06791\n",
      "\tBatch Loss for curent for 199 is 0.07074\n",
      "\tBatch Loss for curent for 299 is 0.06708\n",
      "\tBatch Loss for curent for 399 is 0.07020\n",
      "\tBatch Loss for curent for 499 is 0.06459\n",
      "\tBatch Loss for curent for 599 is 0.07031\n",
      "\tBatch Loss for curent for 699 is 0.07139\n",
      "\tBatch Loss for curent for 799 is 0.06611\n",
      "\tBatch Loss for curent for 899 is 0.06819\n",
      "\tBatch Loss for curent for 999 is 0.07059\n",
      "\tBatch Loss for curent for 1099 is 0.06853\n",
      "\tBatch Loss for curent for 1199 is 0.07014\n",
      "\tBatch Loss for curent for 1299 is 0.06792\n",
      "\tBatch Loss for curent for 1399 is 0.06680\n",
      "\tBatch Loss for curent for 1499 is 0.06641\n",
      "\tBatch Loss for curent for 1599 is 0.07064\n",
      "\tBatch Loss for curent for 1699 is 0.06962\n",
      "\tBatch Loss for curent for 1799 is 0.06441\n",
      "\tBatch Loss for curent for 1899 is 0.06496\n",
      "\tBatch Loss for curent for 1999 is 0.07636\n",
      "\tBatch Loss for curent for 2099 is 0.07473\n",
      "\tBatch Loss for curent for 2199 is 0.06826\n",
      "\tBatch Loss for curent for 2299 is 0.06929\n",
      "\tBatch Loss for curent for 2399 is 0.06424\n",
      "\tBatch Loss for curent for 2499 is 0.06836\n",
      "\tBatch Loss for curent for 2599 is 0.06843\n",
      "\tBatch Loss for curent for 2699 is 0.06730\n",
      "\tBatch Loss for curent for 2799 is 0.06875\n",
      "\tBatch Loss for curent for 2899 is 0.07102\n",
      "\tBatch Loss for curent for 2999 is 0.07481\n",
      "\tBatch Loss for curent for 3099 is 0.07300\n",
      "\tBatch Loss for curent for 3199 is 0.06243\n",
      "\tBatch Loss for curent for 3299 is 0.06651\n",
      "\tBatch Loss for curent for 3399 is 0.06727\n",
      "\tBatch Loss for curent for 3499 is 0.06376\n",
      "\tBatch Loss for curent for 3599 is 0.06409\n",
      "\tBatch Loss for curent for 3699 is 0.06484\n",
      "\tBatch Loss for curent for 3799 is 0.06447\n",
      "\tBatch Loss for curent for 3899 is 0.06804\n",
      "\tBatch Loss for curent for 3999 is 0.06386\n",
      "\tBatch Loss for curent for 4099 is 0.06214\n",
      "\tBatch Loss for curent for 4199 is 0.06487\n",
      "The average loss for the epoch is 0.06789043666203057\n",
      "The average validation loss for the epoch is 0.02093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [1:32:08<3:35:03, 1843.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "\tBatch Loss for curent for 99 is 0.06370\n",
      "\tBatch Loss for curent for 199 is 0.06504\n",
      "\tBatch Loss for curent for 299 is 0.06356\n",
      "\tBatch Loss for curent for 399 is 0.06660\n",
      "\tBatch Loss for curent for 499 is 0.06764\n",
      "\tBatch Loss for curent for 599 is 0.06708\n",
      "\tBatch Loss for curent for 699 is 0.06657\n",
      "\tBatch Loss for curent for 799 is 0.06188\n",
      "\tBatch Loss for curent for 899 is 0.06263\n",
      "\tBatch Loss for curent for 999 is 0.06477\n",
      "\tBatch Loss for curent for 1099 is 0.06628\n",
      "\tBatch Loss for curent for 1199 is 0.06328\n",
      "\tBatch Loss for curent for 1299 is 0.06566\n",
      "\tBatch Loss for curent for 1399 is 0.06188\n",
      "\tBatch Loss for curent for 1499 is 0.06807\n",
      "\tBatch Loss for curent for 1599 is 0.06333\n",
      "\tBatch Loss for curent for 1699 is 0.06973\n",
      "\tBatch Loss for curent for 1799 is 0.06463\n",
      "\tBatch Loss for curent for 1899 is 0.06522\n",
      "\tBatch Loss for curent for 1999 is 0.06396\n",
      "\tBatch Loss for curent for 2099 is 0.06746\n",
      "\tBatch Loss for curent for 2199 is 0.06280\n",
      "\tBatch Loss for curent for 2299 is 0.06327\n",
      "\tBatch Loss for curent for 2399 is 0.06398\n",
      "\tBatch Loss for curent for 2499 is 0.06898\n",
      "\tBatch Loss for curent for 2599 is 0.06391\n",
      "\tBatch Loss for curent for 2699 is 0.06560\n",
      "\tBatch Loss for curent for 2799 is 0.06472\n",
      "\tBatch Loss for curent for 2899 is 0.06524\n",
      "\tBatch Loss for curent for 2999 is 0.06693\n",
      "\tBatch Loss for curent for 3099 is 0.06417\n",
      "\tBatch Loss for curent for 3199 is 0.06469\n",
      "\tBatch Loss for curent for 3299 is 0.06301\n",
      "\tBatch Loss for curent for 3399 is 0.06005\n",
      "\tBatch Loss for curent for 3499 is 0.06314\n",
      "\tBatch Loss for curent for 3599 is 0.06184\n",
      "\tBatch Loss for curent for 3699 is 0.06249\n",
      "\tBatch Loss for curent for 3799 is 0.06107\n",
      "\tBatch Loss for curent for 3899 is 0.06277\n",
      "\tBatch Loss for curent for 3999 is 0.06356\n",
      "\tBatch Loss for curent for 4099 is 0.06271\n",
      "\tBatch Loss for curent for 4199 is 0.07129\n",
      "The average loss for the epoch is 0.06458143834284286\n",
      "The average validation loss for the epoch is 0.02019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [2:02:54<3:04:26, 1844.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "\tBatch Loss for curent for 99 is 0.06234\n",
      "\tBatch Loss for curent for 199 is 0.06207\n",
      "\tBatch Loss for curent for 299 is 0.05934\n",
      "\tBatch Loss for curent for 399 is 0.06355\n",
      "\tBatch Loss for curent for 499 is 0.06158\n",
      "\tBatch Loss for curent for 599 is 0.06246\n",
      "\tBatch Loss for curent for 699 is 0.06189\n",
      "\tBatch Loss for curent for 799 is 0.06263\n",
      "\tBatch Loss for curent for 899 is 0.06058\n",
      "\tBatch Loss for curent for 999 is 0.06039\n",
      "\tBatch Loss for curent for 1099 is 0.06361\n",
      "\tBatch Loss for curent for 1199 is 0.06349\n",
      "\tBatch Loss for curent for 1299 is 0.06409\n",
      "\tBatch Loss for curent for 1399 is 0.06381\n",
      "\tBatch Loss for curent for 1499 is 0.06372\n",
      "\tBatch Loss for curent for 1599 is 0.06118\n",
      "\tBatch Loss for curent for 1699 is 0.06005\n",
      "\tBatch Loss for curent for 1799 is 0.06248\n",
      "\tBatch Loss for curent for 1899 is 0.06529\n",
      "\tBatch Loss for curent for 1999 is 0.06251\n",
      "\tBatch Loss for curent for 2099 is 0.05944\n",
      "\tBatch Loss for curent for 2199 is 0.05846\n",
      "\tBatch Loss for curent for 2299 is 0.06192\n",
      "\tBatch Loss for curent for 2399 is 0.06339\n",
      "\tBatch Loss for curent for 2499 is 0.06099\n",
      "\tBatch Loss for curent for 2599 is 0.06139\n",
      "\tBatch Loss for curent for 2699 is 0.06126\n",
      "\tBatch Loss for curent for 2799 is 0.06107\n",
      "\tBatch Loss for curent for 2899 is 0.05787\n",
      "\tBatch Loss for curent for 2999 is 0.06088\n",
      "\tBatch Loss for curent for 3099 is 0.06311\n",
      "\tBatch Loss for curent for 3199 is 0.05831\n",
      "\tBatch Loss for curent for 3299 is 0.06325\n",
      "\tBatch Loss for curent for 3399 is 0.06160\n",
      "\tBatch Loss for curent for 3499 is 0.06258\n",
      "\tBatch Loss for curent for 3599 is 0.06115\n",
      "\tBatch Loss for curent for 3699 is 0.05929\n",
      "\tBatch Loss for curent for 3799 is 0.06729\n",
      "\tBatch Loss for curent for 3899 is 0.06437\n",
      "\tBatch Loss for curent for 3999 is 0.06437\n",
      "\tBatch Loss for curent for 4099 is 0.05870\n",
      "\tBatch Loss for curent for 4199 is 0.06450\n",
      "The average loss for the epoch is 0.061907327530860284\n",
      "The average validation loss for the epoch is 0.01870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [2:33:37<2:33:39, 1843.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "\tBatch Loss for curent for 99 is 0.05842\n",
      "\tBatch Loss for curent for 199 is 0.06455\n",
      "\tBatch Loss for curent for 299 is 0.05791\n",
      "\tBatch Loss for curent for 399 is 0.06190\n",
      "\tBatch Loss for curent for 499 is 0.05940\n",
      "\tBatch Loss for curent for 599 is 0.05913\n",
      "\tBatch Loss for curent for 699 is 0.05915\n",
      "\tBatch Loss for curent for 799 is 0.06123\n",
      "\tBatch Loss for curent for 899 is 0.06220\n",
      "\tBatch Loss for curent for 999 is 0.06075\n",
      "\tBatch Loss for curent for 1099 is 0.06218\n",
      "\tBatch Loss for curent for 1199 is 0.06095\n",
      "\tBatch Loss for curent for 1299 is 0.06049\n",
      "\tBatch Loss for curent for 1399 is 0.05961\n",
      "\tBatch Loss for curent for 1499 is 0.05969\n",
      "\tBatch Loss for curent for 1599 is 0.06330\n",
      "\tBatch Loss for curent for 1699 is 0.05891\n",
      "\tBatch Loss for curent for 1799 is 0.05940\n",
      "\tBatch Loss for curent for 1899 is 0.05994\n",
      "\tBatch Loss for curent for 1999 is 0.06079\n",
      "\tBatch Loss for curent for 2099 is 0.05803\n",
      "\tBatch Loss for curent for 2199 is 0.06312\n",
      "\tBatch Loss for curent for 2299 is 0.06269\n",
      "\tBatch Loss for curent for 2399 is 0.05628\n",
      "\tBatch Loss for curent for 2499 is 0.06411\n",
      "\tBatch Loss for curent for 2599 is 0.06151\n",
      "\tBatch Loss for curent for 2699 is 0.06359\n",
      "\tBatch Loss for curent for 2799 is 0.05788\n",
      "\tBatch Loss for curent for 2899 is 0.05969\n",
      "\tBatch Loss for curent for 2999 is 0.06072\n",
      "\tBatch Loss for curent for 3099 is 0.05865\n",
      "\tBatch Loss for curent for 3199 is 0.05942\n",
      "\tBatch Loss for curent for 3299 is 0.05927\n",
      "\tBatch Loss for curent for 3399 is 0.06172\n",
      "\tBatch Loss for curent for 3499 is 0.06300\n",
      "\tBatch Loss for curent for 3599 is 0.05898\n",
      "\tBatch Loss for curent for 3699 is 0.05615\n",
      "\tBatch Loss for curent for 3799 is 0.06255\n",
      "\tBatch Loss for curent for 3899 is 0.06275\n",
      "\tBatch Loss for curent for 3999 is 0.06106\n",
      "\tBatch Loss for curent for 4099 is 0.05917\n",
      "\tBatch Loss for curent for 4199 is 0.05880\n",
      "The average loss for the epoch is 0.06044610140120451\n",
      "The average validation loss for the epoch is 0.01761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [3:04:19<2:02:52, 1843.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "\tBatch Loss for curent for 99 is 0.05864\n",
      "\tBatch Loss for curent for 199 is 0.05988\n",
      "\tBatch Loss for curent for 299 is 0.05951\n",
      "\tBatch Loss for curent for 399 is 0.06180\n",
      "\tBatch Loss for curent for 499 is 0.06171\n",
      "\tBatch Loss for curent for 599 is 0.06084\n",
      "\tBatch Loss for curent for 699 is 0.05704\n",
      "\tBatch Loss for curent for 799 is 0.05904\n",
      "\tBatch Loss for curent for 899 is 0.05813\n",
      "\tBatch Loss for curent for 999 is 0.05801\n",
      "\tBatch Loss for curent for 1099 is 0.05988\n",
      "\tBatch Loss for curent for 1199 is 0.05970\n",
      "\tBatch Loss for curent for 1299 is 0.05670\n",
      "\tBatch Loss for curent for 1399 is 0.05650\n",
      "\tBatch Loss for curent for 1499 is 0.05687\n",
      "\tBatch Loss for curent for 1599 is 0.05883\n",
      "\tBatch Loss for curent for 1699 is 0.06181\n",
      "\tBatch Loss for curent for 1799 is 0.05876\n",
      "\tBatch Loss for curent for 1899 is 0.06335\n",
      "\tBatch Loss for curent for 1999 is 0.05928\n",
      "\tBatch Loss for curent for 2099 is 0.05692\n",
      "\tBatch Loss for curent for 2199 is 0.06019\n",
      "\tBatch Loss for curent for 2299 is 0.06023\n",
      "\tBatch Loss for curent for 2399 is 0.05888\n",
      "\tBatch Loss for curent for 2499 is 0.05751\n",
      "\tBatch Loss for curent for 2599 is 0.05961\n",
      "\tBatch Loss for curent for 2699 is 0.05924\n",
      "\tBatch Loss for curent for 2799 is 0.05899\n",
      "\tBatch Loss for curent for 2899 is 0.05990\n",
      "\tBatch Loss for curent for 2999 is 0.05723\n",
      "\tBatch Loss for curent for 3099 is 0.05901\n",
      "\tBatch Loss for curent for 3199 is 0.06155\n",
      "\tBatch Loss for curent for 3299 is 0.06416\n",
      "\tBatch Loss for curent for 3399 is 0.06162\n",
      "\tBatch Loss for curent for 3499 is 0.05808\n",
      "\tBatch Loss for curent for 3599 is 0.05713\n",
      "\tBatch Loss for curent for 3699 is 0.06475\n",
      "\tBatch Loss for curent for 3799 is 0.06246\n",
      "\tBatch Loss for curent for 3899 is 0.05986\n",
      "\tBatch Loss for curent for 3999 is 0.05910\n",
      "\tBatch Loss for curent for 4099 is 0.06100\n",
      "\tBatch Loss for curent for 4199 is 0.06357\n",
      "The average loss for the epoch is 0.05970717172567202\n",
      "The average validation loss for the epoch is 0.01766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [3:35:02<1:32:09, 1843.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "\tBatch Loss for curent for 99 is 0.06064\n",
      "\tBatch Loss for curent for 199 is 0.05973\n",
      "\tBatch Loss for curent for 299 is 0.05812\n",
      "\tBatch Loss for curent for 399 is 0.05666\n",
      "\tBatch Loss for curent for 499 is 0.05612\n",
      "\tBatch Loss for curent for 599 is 0.05949\n",
      "\tBatch Loss for curent for 699 is 0.05568\n",
      "\tBatch Loss for curent for 799 is 0.06225\n",
      "\tBatch Loss for curent for 899 is 0.06159\n",
      "\tBatch Loss for curent for 999 is 0.05966\n",
      "\tBatch Loss for curent for 1099 is 0.05826\n",
      "\tBatch Loss for curent for 1199 is 0.05744\n",
      "\tBatch Loss for curent for 1299 is 0.05760\n",
      "\tBatch Loss for curent for 1399 is 0.05937\n",
      "\tBatch Loss for curent for 1499 is 0.06034\n",
      "\tBatch Loss for curent for 1599 is 0.06178\n",
      "\tBatch Loss for curent for 1699 is 0.06078\n",
      "\tBatch Loss for curent for 1799 is 0.05979\n",
      "\tBatch Loss for curent for 1899 is 0.05551\n",
      "\tBatch Loss for curent for 1999 is 0.05873\n",
      "\tBatch Loss for curent for 2099 is 0.05688\n",
      "\tBatch Loss for curent for 2199 is 0.05782\n",
      "\tBatch Loss for curent for 2299 is 0.05893\n",
      "\tBatch Loss for curent for 2399 is 0.05553\n",
      "\tBatch Loss for curent for 2499 is 0.06207\n",
      "\tBatch Loss for curent for 2599 is 0.06091\n",
      "\tBatch Loss for curent for 2699 is 0.05939\n",
      "\tBatch Loss for curent for 2799 is 0.05716\n",
      "\tBatch Loss for curent for 2899 is 0.06411\n",
      "\tBatch Loss for curent for 2999 is 0.06246\n",
      "\tBatch Loss for curent for 3099 is 0.05859\n",
      "\tBatch Loss for curent for 3199 is 0.05993\n",
      "\tBatch Loss for curent for 3299 is 0.05989\n",
      "\tBatch Loss for curent for 3399 is 0.06097\n",
      "\tBatch Loss for curent for 3499 is 0.05636\n",
      "\tBatch Loss for curent for 3599 is 0.05838\n",
      "\tBatch Loss for curent for 3699 is 0.05767\n",
      "\tBatch Loss for curent for 3799 is 0.05867\n",
      "\tBatch Loss for curent for 3899 is 0.05757\n",
      "\tBatch Loss for curent for 3999 is 0.05476\n",
      "\tBatch Loss for curent for 4099 is 0.05702\n",
      "\tBatch Loss for curent for 4199 is 0.06061\n",
      "The average loss for the epoch is 0.05890585179908209\n",
      "The average validation loss for the epoch is 0.01599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [4:05:45<1:01:26, 1843.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "\tBatch Loss for curent for 99 is 0.06033\n",
      "\tBatch Loss for curent for 199 is 0.05702\n",
      "\tBatch Loss for curent for 299 is 0.05833\n",
      "\tBatch Loss for curent for 399 is 0.06141\n",
      "\tBatch Loss for curent for 499 is 0.06011\n",
      "\tBatch Loss for curent for 599 is 0.05702\n",
      "\tBatch Loss for curent for 699 is 0.05939\n",
      "\tBatch Loss for curent for 799 is 0.05707\n",
      "\tBatch Loss for curent for 899 is 0.05952\n",
      "\tBatch Loss for curent for 999 is 0.05688\n",
      "\tBatch Loss for curent for 1099 is 0.05672\n",
      "\tBatch Loss for curent for 1199 is 0.05764\n",
      "\tBatch Loss for curent for 1299 is 0.05812\n",
      "\tBatch Loss for curent for 1399 is 0.05681\n",
      "\tBatch Loss for curent for 1499 is 0.06007\n",
      "\tBatch Loss for curent for 1599 is 0.05569\n",
      "\tBatch Loss for curent for 1699 is 0.05621\n",
      "\tBatch Loss for curent for 1799 is 0.05894\n",
      "\tBatch Loss for curent for 1899 is 0.06007\n",
      "\tBatch Loss for curent for 1999 is 0.05530\n",
      "\tBatch Loss for curent for 2099 is 0.05909\n",
      "\tBatch Loss for curent for 2199 is 0.05596\n",
      "\tBatch Loss for curent for 2299 is 0.05696\n",
      "\tBatch Loss for curent for 2399 is 0.05868\n",
      "\tBatch Loss for curent for 2499 is 0.05995\n",
      "\tBatch Loss for curent for 2599 is 0.05531\n",
      "\tBatch Loss for curent for 2699 is 0.05791\n",
      "\tBatch Loss for curent for 2799 is 0.05678\n",
      "\tBatch Loss for curent for 2899 is 0.05897\n",
      "\tBatch Loss for curent for 2999 is 0.06022\n",
      "\tBatch Loss for curent for 3099 is 0.05902\n",
      "\tBatch Loss for curent for 3199 is 0.05796\n",
      "\tBatch Loss for curent for 3299 is 0.05876\n",
      "\tBatch Loss for curent for 3399 is 0.06123\n",
      "\tBatch Loss for curent for 3499 is 0.05686\n",
      "\tBatch Loss for curent for 3599 is 0.05892\n",
      "\tBatch Loss for curent for 3699 is 0.06200\n",
      "\tBatch Loss for curent for 3799 is 0.06240\n",
      "\tBatch Loss for curent for 3899 is 0.05830\n",
      "\tBatch Loss for curent for 3999 is 0.05939\n",
      "\tBatch Loss for curent for 4099 is 0.05647\n",
      "\tBatch Loss for curent for 4199 is 0.05776\n",
      "The average loss for the epoch is 0.05836335751440349\n",
      "The average validation loss for the epoch is 0.01735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [4:36:21<30:40, 1840.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "\tBatch Loss for curent for 99 is 0.05955\n",
      "\tBatch Loss for curent for 199 is 0.05719\n",
      "\tBatch Loss for curent for 299 is 0.05807\n",
      "\tBatch Loss for curent for 399 is 0.05899\n",
      "\tBatch Loss for curent for 499 is 0.05899\n",
      "\tBatch Loss for curent for 599 is 0.05730\n",
      "\tBatch Loss for curent for 699 is 0.05764\n",
      "\tBatch Loss for curent for 799 is 0.05614\n",
      "\tBatch Loss for curent for 899 is 0.05741\n",
      "\tBatch Loss for curent for 999 is 0.05531\n",
      "\tBatch Loss for curent for 1099 is 0.05628\n",
      "\tBatch Loss for curent for 1199 is 0.05958\n",
      "\tBatch Loss for curent for 1299 is 0.05854\n",
      "\tBatch Loss for curent for 1399 is 0.05627\n",
      "\tBatch Loss for curent for 1499 is 0.05920\n",
      "\tBatch Loss for curent for 1599 is 0.05833\n",
      "\tBatch Loss for curent for 1699 is 0.05311\n",
      "\tBatch Loss for curent for 1799 is 0.05678\n",
      "\tBatch Loss for curent for 1899 is 0.05790\n",
      "\tBatch Loss for curent for 1999 is 0.05924\n",
      "\tBatch Loss for curent for 2099 is 0.05934\n",
      "\tBatch Loss for curent for 2199 is 0.05406\n",
      "\tBatch Loss for curent for 2299 is 0.05387\n",
      "\tBatch Loss for curent for 2399 is 0.05723\n",
      "\tBatch Loss for curent for 2499 is 0.05965\n",
      "\tBatch Loss for curent for 2599 is 0.05711\n",
      "\tBatch Loss for curent for 2699 is 0.05713\n",
      "\tBatch Loss for curent for 2799 is 0.05614\n",
      "\tBatch Loss for curent for 2899 is 0.05612\n",
      "\tBatch Loss for curent for 2999 is 0.05645\n",
      "\tBatch Loss for curent for 3099 is 0.05439\n",
      "\tBatch Loss for curent for 3199 is 0.05659\n",
      "\tBatch Loss for curent for 3299 is 0.05697\n",
      "\tBatch Loss for curent for 3399 is 0.06030\n",
      "\tBatch Loss for curent for 3499 is 0.05274\n",
      "\tBatch Loss for curent for 3599 is 0.05540\n",
      "\tBatch Loss for curent for 3699 is 0.06013\n",
      "\tBatch Loss for curent for 3799 is 0.05698\n",
      "\tBatch Loss for curent for 3899 is 0.05568\n",
      "\tBatch Loss for curent for 3999 is 0.05533\n",
      "\tBatch Loss for curent for 4099 is 0.05434\n",
      "\tBatch Loss for curent for 4199 is 0.05731\n",
      "The average loss for the epoch is 0.05709351908205222\n",
      "The average validation loss for the epoch is 0.01604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [5:06:43<00:00, 1840.40s/it]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceScore(inputs, targets):\n",
    "    # Calculate intersection and union for each sample in the batch\n",
    "    epsilon = 1e-6\n",
    "    intersection = torch.sum(inputs * targets, dim=(1, 2, 3))  # Sum over height, width, and channels\n",
    "    total = torch.sum(inputs, dim=(1, 2, 3)) + torch.sum(targets, dim=(1, 2, 3))\n",
    "\n",
    "    # Calculate Dice coefficient with epsilon for numerical stability\n",
    "    dice_coefficient = (2. * intersection + epsilon) / (total + epsilon)\n",
    "\n",
    "    return torch.mean(dice_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # visualize the mask outputs\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "    train_img,valid_img,train_mask,valid_mask = train_test_split(total_input_files,total_output_files,test_size=0.2,random_state=13)\n",
    "    #     train_data = SegData(image_paths = train_img,mask_paths=train_mask,img_mean=[129.908, 129.908, 129.908],img_std=[64.484, 64.484, 64.484])\n",
    "    valid_data = SegData(image_paths = valid_img,mask_paths=valid_mask,img_mean=[129.908, 129.908, 129.908],img_std=[64.484, 64.484, 64.484])\n",
    "    model_vis = Unet(3,1)\n",
    "    model_vis_path = '/home/UFAD/s.kapoor/GrainGrowth/skapoor/dl_mis/COVID-19_Radiography_Dataset/UNET/model_3_soft_dice_loss/weigths/best_epoch7'\n",
    "    model_vis.load_state_dict(torch.load(model_vis_path))\n",
    "    model_vis.to(device)\n",
    "    model_vis.eval()\n",
    "    dice_scores = []\n",
    "\n",
    "    test_dataloader = DataLoader(valid_data,batch_size = 1,shuffle = True,pin_memory = True)\n",
    "    threshold = 0.5\n",
    "    for k,batch in enumerate(test_dataloader):\n",
    "        final_image = batch['image'].to(device)\n",
    "        final_mask = torch.unsqueeze(batch['mask'],1)\n",
    "\n",
    "        pred = model_vis.forward(final_image)\n",
    "        pred = torch.sigmoid(pred).cpu().detach()\n",
    "        pred_score = torch.where(pred>0.5,torch.tensor(1),torch.tensor(0))\n",
    "        dice_score = diceScore(pred_score,final_mask)\n",
    "        dice_scores.append(dice_score.item())\n",
    "\n",
    "    print('The average dice score is {0}'.format(np.mean(np.array(dice_scores))))\n",
    "    \n",
    "    final_image_show = torch.squeeze(final_image).cpu().detach().numpy()\n",
    "    final_image_transpose = np.transpose(final_image_show,(1,2,0))\n",
    "\n",
    "    final_mask_transpose = torch.squeeze(final_mask).cpu().detach().numpy()\n",
    "\n",
    "    pred_image_transpose = torch.squeeze(pred).cpu().detach().numpy()\n",
    "\n",
    "    bin_image_transpose = torch.squeeze(pred_score).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.subplot(2,4,1)\n",
    "    plt.imshow(final_image_transpose)\n",
    "    plt.title('orignal image')\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.subplot(2,4,2)\n",
    "    plt.imshow(final_mask_transpose,cmap = 'gray')\n",
    "    plt.title('orignal mask')\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.subplot(2,4,3)\n",
    "    plt.imshow(pred_image_transpose,cmap = 'jet',vmin = 0, vmax = 1)\n",
    "    plt.title('pred map')\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    plt.subplot(2,4,4)\n",
    "    plt.imshow(bin_image_transpose,cmap = 'gray')\n",
    "    plt.title('binary output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average dice score is 0.9839568353069035\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAACSCAYAAAB1wDmsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAy8ElEQVR4nO29e3Dk13Xf+bn9fj/x6gYGGAwG8+LMcEQPaUoMKa5lJopLFmPLluWkIq/LLu1uOclubSqxnFSyTpWzK2/VpipbFSdRObZFx7KilJRYXNuSLTsSI5McDklrXiQHMxgAg1ejG93o97v7t380zuUPmBlM40X0yPhWodDv3/39fueeex7fc64yDINDHKIbWA56AId4dHAoLIfoGofCcoiucSgsh+gah8JyiK5xKCyH6Bo/sMKilPp3Sql/9gEc51eVUv/xAe89q5S6ud9j+KBgO+gB7BcMw/ife2AM/x04edDj2Cv8QGoWpZT1oMfwg4hHRliUUqeVUt9RSmWVUjeUUp80vfc7Sql/q5T6I6VUCfgf1l/7NdNn/rFSalkptaSU+kWllKGUOm76/r9RSv2hUqqglLqklJowffdfK6XmlVJ5pdRbSqlnuxzz80qpBdPzWaXUP1JKXVVKlZRS/0EpNaiU+uP1435bKRU2ff4/K6USSqmcUuoVpdRjpveiSqmX18d0WSn1a0qp75neP6WU+lOlVEYpdVMp9ekdXPYNeCSERSllB14G/gQYAP4+8HtKKbOK/9vAvwT8wPc2ff/jwP8O/ChwHPjofQ7zs8C/AMLA7fXfElwGLgAR4MvAf1ZKuXZ4Op8CXgBOAD8O/DHwT4A+OvfjH5g++8fAJJ1zfhv4PdN7/wYoAUPAz63/AaCU8gJ/uj7WgfVz+w2zsO0IhmH0/B/wLJAALKbXfh/41fXHvwO8tOk7vwP82vrj3wL+L9N7xwEDOG767G+a3v8x4L0txrMGPL7++FeB//iAzz0PLJiezwJ/x/T8a8C/NT3/+8B/fcBvhdbHHASsQAM4aXr/14DvrT/+GeC/b/r+vwf+j93ch0dCswBxYN4wjLbptTlg2PR8/mHff8hnE6bHZcAnT5RS/1Ap9e76cpClc8P6uhz7ZqyYHlfu89y3fkyrUuoLSqlppVSejqCxftx+Os7Jg85pDPjh9SU7uz7mv0NHC+0Yj4o3tAQcUUpZTAIzCkyZPrNV+nwZGDE9P9Ltgdftk18GPgbcMAyjrZRaA1S3v7FD/G3gRTpL5ywdAZXjpoAmnXOSa2A+p3ngu4ZhvLCXA3pUNMslOuvzP1ZK2ZVSz9NZ77/S5fe/Cvz8upHsAf75No7tp3NjUoBNKfXPgcA2vr9T+IEakAY8wP8pbxiG0QK+DvyqUsqjlDoFfNb03f8POKGU+rvr18uulHpSKXV6NwN6JITFMIw68EngbwKrwG8AnzUM470uv//HwP8L/Dc6xutr62/Vuvj6t+gYmlN0lr4qWy95e4WX1o+3CLwDvL7p/b9HR9skgN+lY8PVAAzDKAB/HfgMHa2cAH4dcO5mQOqvIvlpfYZdB5yGYTQPejx7AaXUrwNDhmH83EM/vEM8EpplL6CU+gmllGM9jvHrwMuPsqCsx1HOqw6eAn4B+C/7ecx9Exal1MfXg0G3lVKf36/jbAP/Ex27YxpoAf/LwQ5n1/DTsVtKdGyy/wf4g/084L4sQ+vh9ik6wacFOkGtnzUM4509P9ghPjDsl2Z5CrhtGMaddeP0K3TcwEM8wtgvYRlmo8ewwMYA2iEeQexXUO5+AasN651S6nPA59af/tA+jWNXcDgcOBwObDYbFosFr9eLz+fD6XRitVp1GFwphWEYNJtN7HY7Sr1/+q1Wi1KpRLPZpFar0Wg0aDQa1Ot1arUarVbrAM/wvlg1DKP/fm/sl7AssDGiOELH39cwDOOLwBcBlFI9579bLBaGhoY4cuQIwWCQwcFBnnzySU6ePMnIyAiRSAS/379BOEqlEm63G4vlfYVdLBZ59dVXWVlZYWlpifn5eZaXl0kkEty+fZtUKnVQp/ggzD3ojf1ahi4Dk0qpcaWUg05w6Bv7dKx9gcvl0hqk0WhQrVbJ5/Nks1nK5TLtdhullBaUdrtNq9XaICjtdptSqYTFYsEwjA3aRClFvV4/qNPbEfZFsxiG0VRK/T060U8r8FuGYdzYj2PtB5RSeDweIpEIAwMD1Go10uk0uVxOC5Db7ZZsLrVajUKhQC6Xw+/3a+HI5/NcuXJFv5dIJMhms1QqFWZnZ8nn8wd8ptvDviUSDcP4I+CP9uv39xNOp5OhoSGOHj1KIBCg0Wjgcrnw+XzkcjlWVlbw+/2EQiHsdjs2m41UKsXdu3cZGxtDKUW73abRaGAYBrVaDaUU8XicTCbD2toa5XKZRy16/qhknT9QBINBjh49is1mw+Vy6SVD/orFIsVikUCgk09UShGNRqnX6xiGQbvd1kIlwtFut7FarYRCIYaHh8nlctRqNSqVygGfbfc4FJZNEMO22WxSr9fp6+vDMAxyuRzFYpH5+XnsdjsA9XqdVqtFvV7nzp07LC4uopRiYGCA+fl5ZmZmSCQ6NBm73a7tl1gsRi6XI51OHwrLowqr1UpfXx/9/f309fVhtVppt9t4vV6czk7CdmRkhPHxcQYGBmi1WqyurlKtVrlx4wapVIpgMEhfXx+NRgOlFMFgEJfLhdfrpdlsYrPZKJfLjI6OksvlyOfzNJuPRorqUFjWYbfbOXnyJD6fj7GxMQYHB/VNhk7MpdFo4HQ6cbvdZLNZrVXsdjtvvPEG1WoVq9VKf38/g4OD1GodBoQsXW63G4BQKEQwGMTv92Oz2bh16xbFYrHnheavnLC4XK4NxqcYs6dOndI3LxgM4na7sdls+Hw+XC4XpVIJp9NJPB4nGo3qoJxSiuvXr9Pf38/y8jLJZJJyuUwgEMBut+NyubDZbNjtdiwWCxaLRbvbdrudEydO4HQ6WVlZoVQqkclkqFarerwOhwOfz0cmkzmoS6bxV0JYxBBtt9tEo1Ha7TbFYhG/38/4+DiBQIBIJILFYqFWq7G0tMT09DSxWIxyuUwkEsHr9WK328lms4RCIY4ePUq5XGZhYYEbN26Qy+VoNpv4fD7m5uZoNps0m03tapfLZarVqhYGq9VKq9XC7XZz/PhxHckNh8Pk83kWFxdpt9v4fD7OnTvH1NQUyWTyQCO+P/DC4vF4eOaZZ1hcXKRSqehlJBAI4PF4cDgcBAIBMpkM165dI51OU61WabVa+P1+hoeH+aEf+iGGhoaIxWL09XV42qIpstksa2trFItF2u02fr8fl8uF1WrF6XRSLpfJ5/PcvHmTy5cv65vebrd1AC8ejxOPx3E6naTTaSYmJohGo7z33ns0m01cLhePP/44c3Nz3LlzRy9vHzR+oIXFZrNx8uRJYrEYt2/fxuVyEQwGGRsbw+FwAJ1SmLfeeot3332XRqOx4ftra2usra0xPT3N6OgoH/7whwmHwxw7doxKpUI2m8UwDG7dusXi4iLDw8MUi0WGh4dxOBxUKhXK5TKXL1/mD/7gD7h9+zbtdvuecWYyGaamphgbG2NychKLxUIwGMTj8TAzMwN03Pkf/uEfJh6P8+abb1IoFAC0Z9Zutzfkqlqt1p7HcR5ZYbFarXpmyiy1WCxYrVaazSbtdpvh4WGee+45yuUyTqeTUCjEqVOn8Hq9+oJ+5StfIZlMbnmsSqXC1NQUhmHQ19dHLBbTy9KFCxf4kR/5EV5++WWazSYWiwW3241Silqtxuuvv84f/uEfcufOnfsKiqBer2tD9xOf+AS1Wg2/38/AwACFQgGXy0VfXx9PPPEEk5OTfOc739HxHb/fj8fjwWq1UiqVSKVSLC4usrq6SqvVusdwFuM6mUxuK+XwyAmLhOKtViuVSgWlFKFQCJvNRrPZ1EGxgYEBPv3pTxOJRPjzP/9z+vv7OX78OENDQ7TbbdxuN7/7u7/7UEGRY9rtdsrlMteuXWN4eJihoSG9lA0ODnLy5EkCgQCTk5M4HA6y2Sw3b97k1q1bWrC68XYSiQSXL1/mp37qp2g2m5RKJe2FjYyMMDo6yjPPPMNP/uRP8vbbb1OpVHA4HHi9XhwOB7VajbW1NTKZDOl0mlKpRC6XY21tjUqlwpEjR3j88ce5fPky7XabtbU1qtVqV1rokRIWsTeUUtqYFBtB4iB2u52xsTFefPFFotEob731Fna7nYmJCY4fP47H48Hv9/OlL32JqamphxyxA0kYWiwWXC6XTiwqpXA6nQSDQWw2G7FYjHA4TLPZJJVKMTc3x/LyMqlUqmvD1DAM3nnnHS5dusQv/uIv4na7cTgcpFIpisWijgofOXKEn/iJn2BhYYGFhQXtQTWbTZRSOBwOrUEGBwdpt9v09fUxMjLCzMwMyWSSfD6/LfvnkREW8RomJyfx+/309fUxMDBALBYjGo3idDqx2WxEo1Ht2bzzzjucP3+eEydOMDExgdfrxTAMXn31VQqFApFIhHQ6/dBjG4ahb1osFqPVajE3N0ckEiGfz2uagbjK+Xye+fl5rl+/ztLSEsvLy11pFZfLpZebtbU13nzzTZ599lnsdjsDAwN62a1Wq8zOzlKpVDh9+jT9/f3MzMxgGAY+n498Pk+r1cJqtWo7ze/3YxgG6XQal8vFJz/5Sa5evcrrr7/edUKz54RFlhWfz0ej0aBcLhMKhfjwhz/M6Oiojoh6PB7C4TCBQACbrXMaLpeLer1OqVTi3XffZXV1VQuWz+ejXq/z/e9/n5deeokbN25siGdsBcMwKBaLBINBrfJXV1d1/mdxcZFUKkU8HieVSmGxWFhbW2N4eJhkMkkymXzoDBYbKp/Pk8vlmJ2dJZfLYbVa+dEf/VGCwSB2u51EIsHy8jJ37tzRbvxzzz2HxWLh+vXrrK2t6RiP3W4nGo3S39+vSVhmQ3h4eJiPfvSjLCwsMDMzQzab3XKMPSUsVquV8fFxnn/+ec6ePcv4+Dgvv/wya2trTE5OEggEsFgsFAoFGo0Gdrsdj8dDPB4nHA5rasD8/Dx+v59YLIbP59Nh+ytXrvDSSy/x6quvbstTUErpIN3y8jKPP/440ImJhMNhyuUyFosFh8NBq9XStoBhGAQCAT3bt4JhGBu8sWazyezsLF/96le5cuUKn/rUp/D7/ZruILmldDpNMpnk05/+NB/96Ed56623WFtbw2KxEIlECIfDmmsTCoWoVqusrq5y6dIlpqamyOfzOhXxMPSMsNjtdkZGRhgYGGBxcRGHw8HFixf57Gc/y/z8PNVqlWq1qqOjoVCIgYEBxsbG6Ovro1gskkwmdazk6NGjHDt2jOXlZZxOJ9/85jd55ZVXeOedd7btUiqlGBsbY3R0lMHBQZrNJqFQSBud7XabqakphoaGcLlcvPfeeySTSSKRCPF4nOXlZVZWVra0W6zWTv8h82cKhQLvvPMOt2/fplQq8Uu/9Et4PB6Gh4dJpVIsLCzQarWo1WpcunSJz3zmM3ziE5/g0qVL2oiXc+3r6yOZTPLmm29y/fp1isWiDgzmcrmurkPPCIthGPoCDw0NceHCBRwOB4Zh8MQTT2AYhuaBSD4mHA7T19fH7Owsi4uLXLt2jaWlJdxuN+fPn6darVIoFPj617/O1772NXK53I6zvA6Hg3g8zrlz57BYLDQaDc2rnZ2dJZlMcv78eXK5nFb/AwMDuN1url27dl+3WTSeYRhbClK9Xuftt9/mW9/6Fj/+4z9OqVRiZGSEZDJJqVTCbrfT39/PjRs3uHDhAi+88AKZTEanNNxuN3fv3uWtt97Sk1Cu57Vr17h161ZXBnjPCIvT6WRiYoIXXniBJ598kkgkQqVS0QRpMV4rlQrpdJpisagNVKvVytzcHKlUCrvdztmzZ7UG+Iu/+AsuX76sSdI7QbvdZnZ2lng8TqFQwOv1akENBAK4XC7C4TAul4toNEqr1cJut2thfZBrup3QfS6X45VXXuHUqVOMjo6SyWQ4ffo0c3NzmicTCoUol8vana/X6xQKBU0Ffe6554AOsy+RSHDnzh1Onz6Nw+Hg+vXrW8aBoIeE5cSJE5w5c4bbt2/z3nvvEY1GGRsb4+LFi0SjUW3Ji53S399PJBKh3W5TKBR01NPlcuko6OzsLN/+9rd59913uzZmH4RCoUCxWEQppXNJbrdb3xiHw6E9JqfTSSAQoFKpcPfuXe7evbvr69NoNFheXub69eucOnWK/v5+bDYbyWRSx4H6+/uxWq2srKzQaDSwWCyaySeabnV1lampKer1OqFQiEwmw9DQECsrK6ysrGw5hp4Rlu9///u88847TE5OMjo6Sr1eZ3l5mVdeeYVCocDFixd1ziUej+NwOGi326RSKcrlso6hxONxTQF47bXXuHnz5q4FBdAqXTi25XKZcrnM7OwsN2/eZHx8XGe06/W6XvLsdvueEbNzuRw3b95kaWmJY8eOYbVaGR0d1drVarVSq9VwOp00m02sVqv2jo4eParzV6FQiFqthsViwel0YhgG0WhUR3wfhJ4pjDcMA7vdrlV4MBikv78fl8vF/Pw8hUKBI0eOMDw8jNvtplQqMTU1RTqdxmKxbIimCl1AopR7AaUUc3NzfO1rX+Mv//IvqVarhMNhnE4n0WiUUCiE1+vVXBfxbhYXF+/JOe0UrVaLTCbDzMyMFoDx8XEikQjlcplCoYDVaiWVSmnyuGEYlMtlHeFWSuH3+wmHw7jdbvr6+vD7/QwNDREOh7c8fs9oFpvNxpEjR3j++ed56qmnGBoawmq1opTC6/XqxJh4DfV6HafTqWt3SqUSa2tr2iO5e/cuhUJhz2a1YRhks1kWFxc5cuQI7XabcrlMs9nUtUIej4elpSUd8c1mszpYtpdjKJfLlEol4vE4NpuNXC6nYzSlUkmTsiRLnsvlqFarWCwWbDYbHo9Hl7ZIwE5yaaurqw88fs8ISzgc5tlnn+WFF17YIOkejwfDMLDZbDr4ZhgGDoeD4eFhnbzzeDyMj49rI+/atWtMT0/vmWaBTsEYdGgPNpuNWq1GNpvVRO1gMKhvTDab5e7du5RKpT07PsDS0pIOyA0NDRGPxzl27BgrKyu6YqBer6OUYnFxkcnJSR2Qc7lc2tWWuJBMOCFpXbly5YHH7ollyGKxcPbsWS5evKhzGuJWCkfE7/cD6JuvlNKxglqtRjAYJB6P43a7mZ6e5nvf+96e0xRlPV9bW9NBNqFLStDO5XJRLBY1a26vhaXVajE1NcX09DStVksb9CMjI7jdbtrtNjabDbfbTbFY1Eu03+/H4XBog1dYgmLrhMNhBgYGtjx2T2gWm83GiRMnUErpwSulaDQaOvooWkViHHNzc5qSUK/XN9AOrl69Sr1ex2az7SmzzDAMUqkUpVKJSqVCPp+nXq8zOjqqDd9IJMIzzzxDJBLB4/Fo1txeQmI3Z8+e1RpYjFdZdiVGJY8lM2/WLFLDLUITCoW2PG5PCIvD4WByclLnhGw2m87oikAAG2qKs9ksgUCAWq2mZ4uZz5HP5/fMsDQjl8uhlKJSqeB2u7Wn5fV69XJ4/vx5AoEASimmpqa4cuXKntktknGvVqtcu3aNCxcu6CoCuSbiNns8Hp1QFDdaKaUDeZLnikQiNBqNh06snliGnE6nNsaEqyJ1w0INMKNcLmuGWKPR0Kx5m81GOp1mbm5O5472GpL78fv9mp7p8/l0ok8qEIVs9cQTT+DxePZ0DOl0muXlZRYXFymVSprCKRNLxuPz+cxNk/WkK5VKmsYglA0xirdCTwiLy+Wiv78fv9+vw+jiQrtcrg2MeGlt4XA4NrwnbPupqSkqlQo2m21fNEuz2dTlq6K6xWaRcQCaKScMvr2CYRia+L24uEgikdDXSRKt4s77/X7d5kNqr5PJpC72F+3jcDj0eLdCTyxDPp+PaDSq2WQiKGKsbYbb7SYYDBKJRLDZbHrNleXJZrMxNze3p56QQGaqzESbzaa9Cnif01KpVCiVSjrGsZdIpVJ6Ys3OznL+/HmcTieRSES7yG63W2towzDIZDIkk0l8Pp9ORZgDjT6fj3K5vOVxe0JYhKQjKl6ok+a4ihkej0fPIHGdAf3dy5cv72tZaDqdJpVK6aChuJ3S1KfVaunSD/nbS7Tbbebm5jh69CiFQoFarYbH48Hj8RAIBPR1gI7WttvtWhMppbDZbFpA5BqKhtkKPSEsYpdI3xKZpUJJ8Pl8Gz4r1YFikMmFUUpx586dfW+QUygUWFtbIxwOY7fbNxCwZCxCwZRw+l6j2Wxy/fp1rTUkemyz2bRGFqFptVo6kVoul8lms7qmWzg/wWDwocv2QxdTpdRvKaWSSqnrptciqrOXza31/+Y9cn5lvZ3pTaXU3+jmxCU0rpTSFYAyS1dXV6lUKhsuuNVq1da8y/X+Ti6JRIJ6va6LyvYLkpdKJBK67NTsSUigq6+vT5OP9hoSWrh16xblclkv3YDu2CCP5fNut1t/Jp1O8/bbb/PNb36TdDpNf38/w8Nbt/3rxvL6HeDjm177PPBnhmFMAn+2/hyl1Bk6XZ4eW//Ob6gudhUT1W2xWLSwiNG1mRUvQiMxAnEHG40GmUyGWCy25zbCZkj6YXV1lUKhoGe0QMpeh4eH8Xg8+yIsUv5SqVRYW1vTxw0Gg8D7GliqLGu1GtVqlWKxqGkT1WqVlZUVXn/9dTKZDPF4fMtjPlRYDMN4BdhcaPsi8KX1x18C/pbp9a8YhlEzDGOGTp/8p7o5cSn1lKZ/4tL5/X69LOlBr4eqzRC3T1pl3M/W2QtIIk6itUJLgPcF2RzoyuVy+7IMWSwWvcyk0+kNRCdx4SUC3mg0qNVqup5KYlKSw6pWq7z66qtMT09vecydTsFBwzCWAQzDWFZKSZx4mI0bEnTV0rTdblOpVIjFYtp4FSu+Xq/Tbrf1DZHlaf3Y+jeazSaVSkXHXfarq5LNZuPYsWNEo1Fd/iF2idgrEoWu1+tUKhV9PnsJh8OhGwitrq5Sr9e1Cy/arNFoaO0ihq05JWC32zUpvl6vc+nSpS2Puddxloe2NNUfVOpzSqk3lVJvFgoFnU63Wq1Uq1USiQSJRGJDHfGm72/oFCmFVM1mk1wuty+qH9DZZgm0CS1SZrZhGDqUns1mN3gmewkprne73br6ENB2n2iXarWqta60LxPv0+l0aptP4ldbYafCsqKUiq0PIAZIWd9DW5oKDMP4omEYFw3DuBiNRolGo7hcLu7cucNrr73GzMwMhUJBr73mC7754svNcjqdzM3N6TV8PyAdGDKZjG7kI56GeBuSN8rn89rI3Q9Iew+Hw6GFRQRVvCARZnifwCUZfAn3u1yue4z0+2GnwvIN3t/A8ed4f4OBbwCfUUo5lVLjdDaDfONhPyaRUKlf8fl8eL1ebTTKycP7QTFgw2vNZpN0Ok02m2VgYGBfZrMcK5lMcuXKFbLZrF4WJYjYbrepVqs4HA4mJiZ47LHH+NCHPrSnUVxA00uvXr0KdLLfcrPtdjtWq3WD0EjUW/jM4k3Ka92MrxvX+ffpbOZ0Uim1oJT6BeALwAtKqVt0NnP4AsB6+9Kv0tlM6ZvALxmdXbe2hM1mY3Z2lrt372pJF4+o27XebreTz+e1QbzXN8c81sHBQU0aMkdDzao9EAjQ19fH2NgYTz755EMzujsdS71e14apLHmSOBRBFr6NOAZi4EoIQrzKXRO2DcP42Qe89bEHfP5fsnGb24eiWq3y3nvvbVg/Zc01tz03HQPYGIyTajshHO2Xget0Ounv7+fEiROcPHmSSCSiidxiK4g35vV6deeGc+fO8d3vfnfDb0k4ficQ6obb7dZBws3jlOy02CnmakRz9025jo9E1rlcLmvbRIREoo5yYnIimwVHYBgGt2/fZmlpaV8NXGHRHzt2jHg8rrtZSrBLDG+ZzYFAgNHRUZ5++ul7suC7iQeZlzxgQ6GYmd5hrnQ0L0uVSkU7BUIPfVgisSeEBdDhe3OvFeFu1Go17UKbtYrZhS6Xy8zNzZHL5fY027zZ9hkaGmJycpLTp08TCAQIh8OaliiQpUgizIFAgPPnzzM4OLjht3bTwUn4uNChHFSr1XuCl2aCk9gxQh6ToFw2m9WlIw/LDfWMsEgTnlqtpg3WVquljbNSqfTAiytc2FAopFXrXsEsLFarlQsXLnDixAktDLIcyNgk0CUBOxF6v9//0AjpdlEul8lkMpRKJYrFotbEZsahJFrluQiRBO2kg5WkSrZCTwiLqEaJCcgskTiBCJEE6sR4Fe2Sz+d11V83zXm2A/Ny5nQ6CYfDmrIoN0GiqfJZ0XhCmZC0gLRJ3cuxLS4u6mNttlvM2XiJkjebTa1ZisWivr7SEWIr9ISwSLRRds2QZUS0hNS7mJci84xPJBKUy2Xu3r27r90ca7UaCwsL2sYyDEMvATKzzRAX1W634/f7NdVyL1GpVHTWeXORmCzpEtoHtA1YqVS0ASx0iocVyPcERUHUoWgYmal2u1034JFZW6/XtWqXC+B0Opmfnyefz+9LaF2glNKFZZIJFx6O+WbIY4/Ho+ufXS4XExMTOJ3OPee3zM3NoZTSQTqphJDlXCaZaBVpjLi2tqZrq7rh4PaEsLRaLR3gEspfvV7fwOqqVqs68rh5drbbbW7fvg3AwMAAy8vLux6T2dU0Qzi3ou1EuDe7wWK3SIuudDpNX18fQ0ND3L17d0+9NTFUG42GjjXB+96WLOPiPYk2kepGSXYKl/lB6Alhabfbur+ZBIfMQSS3263XWbPXIVhcXKTVahGLxUin0xs8pZ1CLp5hGJp1J/1W/H4/0WhUk7Ae1CVBOK7yex/60Id46qmnsNlszMzM7JkGDAaDJJNJrUUEZk0iS490oVhZWeHGjRu6dSrc6/ltRk8IS6vV0u6b0CllxoqgSA2R9NM3u3m5XI56vU4+n9dRyd16RBaLhXA4vIF1d/z4cb3lnc/n05lcMXjvFzeR5J1oysHBQSYmJqjX6ywsLOxaw8jejaVSiZWVlQ3nbc5ZCUmrVqtRKpVIJpP3MAofNsF6Qliazaa26n0+H+12WxN2pBuAuQ7GPANEA0ia3WKx7LqoSwJryWRSG60iGNLHX4S52Wxq49bn822ggJp/Tyotjx07pu2wer2ut5jZzVglU7+4uKhDD6Jd5VqKMSuGbCqV2rbt1BPCIpQEaV+eSCQoFAoMDw9z8eJFAoGAtgkGBwc35H0ajcaGFlsrKyu7nq2SJTZvrmC1WgkGgxw5coRQKKQ9DEnMSW7oQTDnqzKZDKOjozSbTdbW1nYVnJP8kDRE3NxEsNFoUCwWdT9csVeko+W2jrXjUe4hms0miUSCarWqo7WiPU6cOMHq6irFYpGJiQlNV5CZs7a2poNRy8vLunh9p7Barfj9/nsaBvr9fk6fPq33IYKNrqmZ63o/yOaa6XSaQqFAOBzmzJkzzMzMbLAbtgtZsguFAplMZsP5i93ndrvJZDJaSPL5/D285m7QE3EWwzBYWVnRm02aWfvi1ombKrkh+Usmk3rGJhKJXRu27XZbM8/MqNfrRKNR7WKaXXd4eJ5H8jH9/f0MDQ1RKpUIBAIMDw/vKvYiHBrpQinXCtDpBtHYEvCUa7pd9ISwSLTWDGkwKCUV0Wh0Q/gc0GUQEindC+/C5XJpA3bz62q9V4yZ4il42A23WCyEQiH926Ojo7qbwW7pFHLu0p/FPKbNsStzAHS76AlhuR9kUya3261PTC6qCIzwN2ZnZ1laui8hb9uoVqssLy/fEzqv1+ssLi5SrVY1gVxuRjeaQWqQJQt99OhRrS2Fkb9TCPHLYrFsoKCaJ5Z8TigLO9FmPSssMhNE48gJmikKkmC8evXqnu2RbGbFmyH97GR73u1ebClrFY5LPB7n9OnTeDweTp06dV8vajsol8sbwgxqvXDPbN8BD7WttkJPGLj3gxB2nE6nLleQLLQZi4uLTE9P72kPlM3C4nQ6OXr0KMePH7+nLEWw1cWX96SNvMVi0X3dJiYmsNlsHD9+nOvXr+/4POr1OtlsFrfbTT6f14X65usouSpgR0tfzwqLOcdiLkY31wPJ0iBbzu1F5PZ+ED6r9IPZKTweD7FYjGq1qnNhjz32mNYAKysruzLSM5kM7XZnT+mBgQFdJ7R52dm8PHWLnhUWab0hATIxMM1WfDabZWFhYd8olAKv18vY2Bj9/f07NkYleBYOhzXnWHY9k6VvZWVFbz+zUxSLxQ3ZYzP7ENDG7k6uWc8Ki2RMzQakeSs38YRkedpPuFwuhoaGNCVBitC3A6FeCqkrHA4TDAZ1p6ZMJsPAwIBuUbrd6KrFYqGvr49UKqW9S/GE5H1hIgI7Clz2rIErZCi5uMJrgfdJT61WS1v/93O/9wqpVIrZ2Vmy2Sz5fH7HEWJJXYgRCujYUSQSYXJyUjd93m75rcPh0D1bzNpPjHFh9JuX9O2iZzWLtFk3t4uQJUnez2QymtK4l1TK+41F8iuSH9oJLBaLPg+JdwA63nLy5Ensdjtut5srV65w69atrn/b6XTqPnrS4lRsLfEsgQ110Nse/7a/8QFBKv+k7bksQbIkVatVUqkUNptN1+3sFwzD0H3XfD7fruyWWq2G1+vdkF2XrPDs7KymEggbr1sINTKdTrO6uqq5t+aEotgqEmvZLnpWWOSkisWiTgEIW73ZbLK6ukoymcQwDL1t7X5BCss8Hs8DXeetIDfLnCqQakCpDvB6vTrHJM2it6PBROiSyeQ9yURZcmQMhULhB0tYAJ0ozGQyGxKMFouFTCajU+37vSm2dEYaGBjYca2PVCCIQEiLEaUUkUiEQCBAf38/SnV2HTl+/LjebaTb35dk5fz8vL4mQh4TwZMc1U7Q08LSaDQolUq6kZ94FCIsqVRqz9n894MseU6nc8ftUoVxJwVwdrtdM9ckOixJv0wmo1Md2/l96ZKVSqV0HAfer04UAtlODfSeFhZxn2UZMgfpqtUq09PTD90Ecq/GId2dPB7PzvIqpr1/hLVm3opG6rrNS50Q1ruFbIYlSUVJUsp4xUjfaaihZ4VF4ivNZlOXV4qBK5xd4dvuVxG8jEO02U6MWzFipZGOtESXlqdSkmEmR4nH4vV6iUaj2xIY2chTNK54Q+bNKXYqLD3rOkuNrkRyxW0VKuPKysoDGfh7DSE8d2s/CMy8G6vVqo1a4Z7UajVd1C6RV+HMJpNJ7dU4HI6uN4yQwrHV1VV9XMkNeb1eyuXyjm28nhUWc4ZZwvzmbkb5fJ5wOPzAQvluYI4/POh9u93O6dOnGR8f1yUT21mGhMooO26YW1sUi0UtMHKDW60WTqdT0yBFULuFw+GgXC5vIEGJNybE8p0mKx8qLEqpI8BLwBDQBr5oGMa/VkpFgP8EHAVmgU8bhrG2/p1fAX4BaAH/wDCMb213YGZBaTabG9qdSjCuVCp1fSEDgQCxWAzDMLTL6vV6qVQqei0vl8vkcjndGOfChQucOnVK7y+43YrCZrOpmxpL3bF5KxyxMdLpNLVajdXVVb1rqtzY7QYcpT2ZsOZEG5sDdDtl5nWjWZrAPzQM422llB94Syn1p8D/SKe96ReUUp+n0970l9XG9qZx4NtKqRNGF019NkPsEfEeZP1fXFzE6XQ+dK9kqTvyer0cO3aM0dFRHnvsMeLx+AZVvJn5Pj09jVKKc+fOMTk5idPp1PtHdwtZRovFIg6Hg+XlZex2O8ViUbP77Xa7ppJKP9t8Pq83+t5pCqNUKpFIJHS2WZoQSrphp5q4m2Y+y4B0piwopd6l04HyReD59Y99CfgO8MuY2psCM0opaW/62nYHJ26qzAqZjSIkW5Vc2u12YrGY3rx7YmKCUChEf38/DoeDvr4+gsGgZsQJP3V5eZmhoSEmJiYoFot4PB5Onz6tN/HczqwUmqMYnI1GQwffUqkU9XqdtbU1Wq0WDodDC8na2prOQ23HGJXlWmI6UnEgtovwcD8QA1cpdRT4EHCJXbY3VUp9DvjcVsczh6klciskn1KphN/vJxQKsbq6usH2CAQCnDhxgqNHj3L27FkmJiZ0lleWN2kMGAgEtDtZq9UYGhrSPe1kiTLvVNYtDMPQpRmyj7IY7BJsFK9IYi+NRoNwOKw3kBCbrFaraZdbOkbcD1LqWyqVtLYMBAK6J68I0r5pFoFSygd8DfjfDMPIbzHDumpvahjGF4Evrv/2fUcvN1Y2K1BKUSgUtAs4ODi4gXurlGJiYoLnnnuOiYkJotEosViMaDSqw/USr/F6vXqDBNmFTDbplKRcIBDQ3oO5xXm3EA0hWrBYLLKysoLD4SCbzZLL5XC73fh8Pi3QUl1os9n0GOfm5iiXy6TTae7evcuf/Mmf6E00zZBorbl1iUw4c1Ok/bRZUErZ6QjK7xmG8fX1l1eUUrF1rbKj9qYPg7TaEsPWZrNpF7Jer+sO3NJv9ty5c7z44oucPXuW0dFRXSEgkVHZJQ3e55cI3RDQATcJwgkzz0xH7BZSgGYmfotmrNfrDAwMcO7cOUZHRxkbG9P7FknBmlQslkoljh49qrVTIpEgGAxy+fJl8vk8d+/evUcAZMtjc2Mk0Tj7Kiyq88v/AXjXMIx/ZXpL2pt+gXvbm35ZKfWv6Bi4XbU3vR+kq5LEN2SDyEQigcPhwGq1am0Rj8f5mZ/5GU6ePMmJEyc4cuSIDnPLJlaby0hEOOQ180V0uVzam5D269vFkSNHWFpaolgs6nZhY2NjTExMMDg4iMvlIpVKYbFYdJDRbrczODioW9NDJ1wvQhYOhxkdHdUF+U8++SQ3btxgenpa20RS0C+ZeDFuRUPup2Z5Bvi7wDWl1PfXX/sndITkq6rT6vQu8NPQaW+qlJL2pk26bG/6IMgSJMtEu91maWmJZrOp7YFgMMgnP/lJzp07x/DwMLFYTC8tgvtdoAeVcchrZm2y3Qus1nu5PP300zqCKkFEEbxsNqvLSWXL38HBQZ11rtVq5PN5ms2mtlXsdjvxeJyf/umf5rXXXmNwcJBnnnmG3/7t3+bq1atawM31QZVKhWKxuOvgZTfe0Pe4vx0Ce9je9H4QG0FmyeY0/NLSEo1GgxdffJGPfOQjDAwMMDIyoovTumHcb4XdVArK+CORyD2/ZRid1mbf/e53WVhYIBaLUSgUGB8fJxaL6Z1k5RyEESiGbblcZnR0FKvVSjqdJhaL8fM///P85m/+JjMzM7p1iVARJCxg3t9gJ+jZCC6gmw9KSF+M00ajwfT0NKVSiXPnzvGxj32Mp59+WjPwdxN42iuIcX6/XFKtVuO9997jjTfeoFKpMDs7q7sxSMMfiY8MDg7qkL/0pZN27y6Xi9HRUVwuFx/5yEeoVqt8+ctfZnl5WXellOVHto4R7bwT9LSwCIOsXC7rNhcul0tTLv1+P+fPn9cF6+bNIXsBDxqHUkpvQReJRGi1WmQyGW7dukWhUODjH/84586d08lHmShCkkokEpp1J9xat9vNU089xczMDN/4xjd0lwfRxplMhnw+r6keO0FPC4uQtuViNhoNTW+s1WpMTEwwPDysd241N9o7aDxoHJLbEma/dKxqtVosLS2xtLTE7du38fl8HD9+XHtnkiZYXV3dQL6Wa9JutwkEApw5c4alpSVNRRWbSJaf3dgtPUtRgPfLFaSGV9qeSw/a4eFhgsGgDjr1iqA8DHa7XUeJxZ2X1mORSER7P8I7XllZ0YQmIWWLJyfpBKU6m5COj4/zqU99CqvVSj6f140DJCG5m7KZntYsMnMki7q2tsapU6fweDyMjIwwNjbG2NiYbvTzKAiMUp29CiORCB6Ph3A4jFKKUChEOp1mbGyM8fFxvYu71WqlUCjozb4lZuR0Ounr69OhBWm9EY/HGRkZ4c6dO9pbLBaL2vXeDbG954VFGvxI6YeEwU+dOsVjjz2m9yusVqt73pR4P+FyuQiHwzoFIHGdU6dOMTExsSFaXCwWeeONN/D5fDz++OPEYjFdNy2Jx1AoRKVS0S1BHnvsMZaWljSh3dxTeKfoaWEBNMNfuLjtdptYLEYwGGRwcFALym4ojwcBr9fL0NCQ7jLp8XgIhUJ6E87NEE5tIpEgHA5TKpWwWCz09/eTy+U00VtsujNnzugSGemqAA9vMrgVel5YKpWKjhM4HA5qtRr9/f00Gg0dIoeddQU4KAhVU7aiSafTAJw9e5YjR47cI/CRSISBgQGq1Sqzs7M6sRoKhRgZGSEYDOrIrHg/fr+fM2fO6JLVfD6vub87Rc8Li/RgEcZZPp8nGo3qVqiSw+kll7kbKNXZn+jEiRN6f6IHncPAwAAnT57k2rVrrK6u6r52pVJpgyAJ/VR4Mm63W3ellCjubpoz9rywAHoJqlQqFAoF4vE4brdb50/MCcJHCVKD/LCxy3bAtVoNl8tFIpEgGo1qrWLe/dVisWjhgPe3MDa/tlM8ErpbaI+iRoVi4HQ6abfbG3ZC/0FFKBTSNAu3200wGCQWi+lNvqV5s7mtmoT3heO72xLfR2I6ym7nskeyqNxQKEQgENjQ7vQHEUophoaGOHXqFKurq4RCIV1vJJpJWPziJgOa9yM0hd22JnkkhKXVapFIJBgaGiKTyWC327XdEolEHjl7ZSdwOBycPHmS48eP65bqkoU2u9kSuQV0fgjYk30jHwlhAfR+Qnfu3NG7bUiiDHafIe5lyLlJW3rJBUmzAPPnzJ2/haUnAbvd4pGwWQSlUomFhQVN8gkGg38ltMr9IIVjm/de8nq9eqMLCdjtpJPU/dDTmkWIPgMDA8RiMSYnJ3n88cc5ffo0w8PD+Hy+HTfWeZSxFQXD7XYzNDSE2+0mGo0yPDzM4OAggUCAq1evMj8/z507d3YkPGq/Sz+7GoRSKaAErB70WO6DPnpzXLA/YxszDKP/fm/0hLAAKKXeNAzj4kGPYzN6dVzwwY/tkbJZDnGwOBSWQ3SNXhKWLx70AB6AXh0XfMBj6xmb5RC9j17SLIfocRy4sCilPq6UuqmUur3euuODPv5vKaWSSqnrptciSqk/VUrdWv8fNr33K+tjvamU+hv7OK4jSqn/ppR6Vyl1Qyn1vx742MytrD7oP8AKTAPHAAdwBTjzAY/hOeAJ4Lrptf8b+Pz6488Dv77++Mz6GJ3A+PrYrfs0rhjwxPpjPzC1fvwDG9tBa5angNuGYdwxDKMOfIVOf5cPDIZhvAJkNr38Ip2eM6z//1um179iGEbNMIwZQHrP7Me4lg3DeHv9cQEw98U5kLEdtLAMA/Om5/ft5XIA2NB7BjD3nvnAx7tVX5wPcmwHLSxd9XLpIXzg493cF2erj97ntT0d20ELy571ctljrKz3nGG/es90g6364hzE2A5aWC4Dk0qpcaWUg07jwm8c8Jjg/d4zcG/vmc8opZxKqXF20XvmYeiiL84HP7aD9IbWrfgfo2PpTwP/9ACO//t0Giw26MzOXwCiwJ8Bt9b/R0yf/6frY70J/M19HNdfo7OMXAW+v/73Ywc5tsMI7iG6xkEvQ4d4hHAoLIfoGofCcoiucSgsh+gah8JyiK5xKCyH6BqHwnKIrnEoLIfoGv8/TV1Y2BJfDGcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAACSCAYAAAB1wDmsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ2ElEQVR4nO2de3RV1Z3HPz/x8igoGOSNBRRUGLBgmTIOMLCKjo9FC8NQBh+McQkMCKhFbZDYcq1NK6xCYaC8C0FhwIwwlFKxijNocaiKGCsQXsFYYEJ4CagFBfzNH+cEbsJN7rm5Z59zz835rJWVc89j/35Jvtl7n73377dFVQkJccIVfjsQEhxCsYQ4JhRLiGNCsYQ4JhRLiGNCsYQ4JhRLCohIiYjc7rcfXhGKJcQxoVhsRORKv31IdzJaLHYz8bSI7BSRT0VkqYjUt6/1F5GDIpIjIoeBpSJyhYhMEpFiETkuIgUikhVT3ggR+cS+lpvAdr6IzBWRDSLyuYi8LSItRWSm7csuEekRc3+53c9sf/8p5lq2/fxsETllPzvAwK+sWjJaLDb3A3cCNwA3As/EXGsJZAHtgNHAo8BgoB/QGvgU+DWAiHQB5gEj7GtNgbYJbA+z7V0LfAlsAbbZn18GZsTcWwz0BRoDzwLLRaRVzPVewH772SnAmlghe4KqZuwXUAKMifl8D1BsH/cHvgLqx1wvAgbEfG4FnAOuBH4CrIq51tB+/vYqbOcDi2I+TwCKYj53A05W43shMMg+zgb+D5CY6+8CI7z8fdaGdvpAzPEnWLVCOUdV9WzM53bAf4nI1zHnLgAt7OculqWqX4jI8QS2y2KOz8T53Kj8g4j8KzARaG+faoRVi5RzSCvO+lb+WYxTG5qh62KOv4n1H1pO5Sn3A8Ddqtok5qu+qh4CSmPLEpFvYDVFKSMi7YBFwHigqao2AbYDEnNbGxGJ/Vz5ZzFObRDLOBFpa7fvk4GXqrl3PpBn//EQkWYiMsi+9jIwUET6iEhd4Ke49/triCXco7bdh4Cule5pDjwqIhER+QHQGXjFJfuOqA1i+Q/gNazO4X7gZ9XcOwtYB7wmIp8Bf8LqWKKqO4BxdnmlWJ3fg244qKo7gelYHeAyrP7M25VuewfoBBwD8oChqpqoGXQVyeTFTyJSAoxU1Y1++5IKIpKN9XP08dOP2lCzhLiEMbGIyF0isltE9onIJFN2QrzDSDMkInWAPcAdWO36e8C9dtscElBM1SzfAfap6n5V/QpYBQxK8ExImmNKLG2oOBh20D4XEmBMjeBKnHMV2jsRGY01HwNEvl1xsDLEP0qPqWqzeFdMieUgFUdO21JptFFVFwILAURa60XdhPjMs59UdcVUM/Qe0ElEOtijncOxBrtCAoyRmkVVz4vIeOAPQB1giT0CGhJgjM06q+oreDx3EWKWcAQ3xDGhWEIcE4olxDGhWEIcE4olxDG1YQ1uEkSA5tBzlLUCdhNY64zOuWijQZxzZ1ws3xyhWAC4GsZPpGR2c9qNOUr+gtGcBoYAbTdBq37FHJaGWJEgNWUsA/Utfjd9GJyIOX0lPPLsdObdPRFenQGcTsGGWdJipZy/w/0/okTbs1OO8k4Vd0SA3O7Q94PX2CxFVPxrJyKL63UAxT26kldYdR11O9BaW3KD7KZiOJHXPPu+qvaMd6V291nWR9GuDVlajVDA+gNHC+E++UdGq2LFpTkhi/F6lielK9HC6huzjcALchjt3hhejjos31tqqVgisDbKhwOF6HbnT5UBt8njsPVRR/e31IHcIjkVgoUSES2EHUMFVkWTeMobaqdY1ueyY7CwpgaPlgB6nQBXJ7hzIqV9buBQDWwUAHuGS9rVMLVMLBFYb9UoBakU8xDAVdXecr0eIK9yMEcSrMCuYdJIMLVLLOtz2TGwZjVKLO+/ArQfVc0dLSh+u2vKL9wFwJ40EkztEcvwKHtSrVFsXgVrhU6VDGGnSxE+F2uYtVF3CkyB2iGW/Ch6QVjhmcEsV4fZCoAdgwVmRl0sNXlqgVgmoluE6H96Z7HR5ydxOwSyANBTgh1N6wsZLpZh6PzGRBd4a/WGhsVGBvDnTYF/9ja8uQIZPdx/mx4mL16cQUApA5bUH8FqFIh6bj+Da5b7+d/+A1ydAkwHZnwJU3UC1iSEt2SsWPppMXlvGiobYL2Zsp0wvtEcoNr8h0bIULG0YNO0u43VKp3rYOVl8olpX8B0fcRzu5kplufHsjzHXPHLLmAl9KqCzxKM7rrBhCbzgB8ZtxNLBoolwoac/uwzaGFiL2DO61Ve3y91if4FOhr0Yd4pYNc3DFq4nAwUS2/u6myos1JOU7CmFKtiBfLNWTylxUwxlKn2BHDkpqtIPKHpHpknljn9mbHLbycATnBYXuCW4+9ezFXqNs1afI6Vl9kbMk4s74/rklYLE7fLDrJ/a6bs6BFgTqIk3+6RYWLpyK0Livx2ohIlyFE11ljMHZdtqOTLyTCx3M+mMX77EIc58df0u8HYMcuA3oZKr0hmiWW8JLWEscbU8cKIMzYvAGsbAvNklFgGzV6JF43Qh+s6kcwK/3Yf7DLWjyoCWOvN0H9CsYjIEhE5IiLbY85licjrIrLX/n5NzLWn7XSmu0XkTlOOX87VrF1+nyeWfsUPcR4YFmERo4yFkR0C/jjo23gxV+SkZskH7qp0bhLwhqp2At6wP5fvyTMc+Bv7mbl2mlMPuB2mmrcSAfZwUxJPdOOOHptNuQNAn1HbMNcrukRCsajqW1xe5w4CltnHy7A2dCo/v0pVv1TVj4F9WGlOzTPyFqZ6MF9zM7Dluu8m8UQEDC9B+etKgB+aNULN+ywtVLUUwP7e3D7vX0rT6FnOe2AmAi5t7+Aes78AHje/cMftDm7ClKYXbxQZLSJbRWQr/DVlw4VtbvFk7Uryo/clxlMvngFe/NVQs0aouVjKyvfvs78fsc8nTGlajqouVNWeVlyttxNiqdDnRUguQL6Mn3V/wpA3l+hodOrUoqZiWQc8aB8/CPw25vxwEaknIh2w9sd5NzUXndCCb+3aa94MWJ2WJEdzfvy9Xxpvi//uyQ+BW43acPLqvBJr06Sb7K1tHwaeB+4Qkb1Ymzk8Dxc3cCoAdmKF14xT1QumnL9EPw52Nm+lxqxfzqh/M2vi+EzA2JSlRcIF26p6bxWX4u4rrKp5WBlwMpL+PTdgbXCWDPuYPP/H1F3wnAmXADh/AUyvv8+QEdwuSWVMSYU3/1B5yMkZv/jwp0ZXnmwCiHYxaCFDxNL47OGU45eN030vExqbK74IaPTkUXMGyBCxNKiX+qu3eQo4ftKgWoABDc1uBZkRYindfoMndq6G6ldTVss5Wv3+pNEZnLWLzc6NZYBYItZ2Eh4wsTUwJoV5noEHya28W7ObvAkmY6EzQCweUg+SSz5YmcUM/ehFY1N+i5YDHe82VHpGiCUrtb+fx6yWBuQY6o2fAaMh0MEXS9uxrPu5R7b6QeoJjj9CpinRuMlDU+MEMPN+c6N/wRdLE2qU5K8mLFr6ALixFu9PUeq+espoEJoJgi+W+t6ZOu/i4ttz1y5m6OeuFecJgRfL6PdmebJIuwEwl3EulniaBo2+ItrBxSKBxwoXAi3cLdQm8GLpyx89sZMFbO/xty6Xmkf2/rnuTgMMA1PrzQIulm48cOdq78wZWIq3TBowcY/75Zog4GKJgEfLWPqDoZwsJcgwZYiJol0m2GIZ/H02fOyNqU5TwVj4QOFUntK1rhT1+l5g4PddKasywRbLGKrdzcNV6oGRdgiAM7wmTYn+PvWS3gYYmXo58Qi2WDzk9PgI7u5oVpmNyIvqTj6XJi6UEYdgi6XrWc9MdcCD9m5VlCve/boGEQQVKe3XxA1vLiPQYjnTxHwUnud0nM8gbZZSES23n3LJmYoEWiz1PUrF0hs4MdibWDkoo70cIdrdI3NJEGCx9AaP0qzfArB+pzfGAIjyvQ8Kar6UYT6YyNkSYLHcwbrFfvtgjvXSiJwaphfb8GuwInTcJbhiWZVsqFfQeAc5oIYjgZIjsGJ58F/mebY0wTfGLyLboyWjTgioWCL08m44zkcOIXuTr11KANc3PCKwYmnO2CHLEt/mEkUAfcwGcFXJ+Hyylyb3SBlw74AlrrsSULEAX3pnahPQeONh7wxWoIS/z34j6afqGvgFBVQsoyh7xTtrDYBTJS29M1iJLXIjuUnGp+VPf4SaZJOpjmCKZRJ4+dY8DOBmP9+9Cjh5slFyj7wJbiclDOa2dzXwug0wqh/27PHlnHsH8uKMkkeADkVA503JG3WN0zzNL+jABF93ZhPVuFm8Lt0gch3wAtAS+BpYqKqzRCQLeAkrKUgJMExVP7WfeRp4GLgAPKqq1b4AirRWGO3Y6blaQpk46+COBLboQIY99DvIfwmqzEibDVHrP/GZKZN5lNkcpymdV5fA0HeADY79M0LPKAu3iuPhgl7APX0UNkeTNPTs+1Y2rstxIpZWQCtV3SYiVwHvY2WnzAZOqOrzIjIJuEZVc+z0piuxslS2xnqJu7G6pD7JiqVUZzJfqp8siwC5j4GsVjiYh9nlBV4wBO33LaIOd8eJAM88qfDLaJJ2qhaLk9Smpaq6zT7+DOtNsg2+pTcdQsvO1QslC2isDyOzDsDBKMEXCsAa+Im/HiTVwRWR9kAPrAVqKaU3rXm2yhbsrGY/oSzgvI5lgvTC226weSRHiS5KIj1ykn3iRDgWi4g0AlYDj6tqdanoHaU3NZGtslwoT0g3vItT9JCtUWTUKvbq7ITxRueAtVPczYbvSCwiEsESygpVLQ/rTjm9aY14pgUfxTkdAZrpIJ6QHmT2FGMRv5FjyMenqKeP062aO7vxZ1ctO8lWKcBvgCJVnRFzyZf0pp2f2xY32jj3MbhPFpCRNUpcZjBZOjB0sboe1VgVTkYsegMjgI9EpNA+NxkrnWmBner0L8APwEpvKiLl6U3P40F607HYbz0m802kJSdgZBQZrChC1PAyYSepTTcTvx8CaZDetDfQ8uAZaJux2VQTszaKDDcvmGAO99tEgI0ahbbTyYzX4xRYFUWeU6OZvAMnlqK+t15cXZrbAabJU9R6oZTzwL+zU39uLN9u4MTC5rKL/z2H9mcB0/z0Js04wUzpy8Q51qfrDxzGzU3CgycWmwZA29WGd30KJBuRrXag/RqAdq6VHECxnCMC5GQDQ81uJxdY8vN4QZ9j72NtIe6oVM0I4BKFNXQqgpybo5BvNqN0cDnHdMnlmDbFzQHKANYsJ/jvm29jWt8pfjuS5uSxzOWd8AJYs8AAWQEs8tuNAODutEcgxXJpZUSIlwSwGQrxi1AsIY4JxRLimFAsIY4JxRLimFAsIY4JxRLimIRxQ544IXIU+AI45rcvcbiW9PQLzPjWTjV+BsS0EAuAiGytKrjJT9LVL/Det7AZCnFMKJYQx6STWBb67UAVpKtf4LFvadNnCUl/0qlmCUlzfBeLiNwlIrtFZJ+dusNr+0tE5IiIbI85lyUir4vIXvv7NTHXnrZ93S0i7gYTV/TrOhH5HxEpEpEdIvKY776pqm9fQB2gGLgeqAt8CHTx2Id/AG4FtsecmwZMso8nAVPt4y62j/WADrbvdQz51Qq41T6+Cthj2/fNN79rlu8A+1R1v6p+BazCyu/iGar6FpfvOe9T7pkKfqVZXhz/myFHuVx8IKXcM27jZl6cVPBbLI5yuaQRnvvrdl6cVPBbLGZzudQcf3LPVCKt8uLgv1jeAzqJSAcRqQsMx8rv4je+5J6JJd3y4gD+vg3Zvfh7sHr6xUCuD/ZXAqVY0fUHsVKyNgXewNo1+g0gK+b+XNvX3cDdBv3qg9WM/BkotL/u8dO3cAQ3xDF+N0MhASIUS4hjQrGEOCYUS4hjQrGEOCYUS4hjQrGEOCYUS4hj/h9lcsHlwYR6EQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[129.90850553548862, 129.90850553548862, 129.90850553548862]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64.48403222822402, 64.48403222822402, 64.48403222822402]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_model_summary as pms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------\n",
      "       Layer (type)                                  Input Shape         Param #     Tr. Param #\n",
      "=================================================================================================\n",
      "       DoubleConv-1                            [32, 3, 256, 256]          38,976          38,976\n",
      "       DownSample-2                           [32, 64, 256, 256]               0               0\n",
      "       DoubleConv-3                           [32, 64, 128, 128]         221,952         221,952\n",
      "       DownSample-4                          [32, 128, 128, 128]               0               0\n",
      "       DoubleConv-5                            [32, 128, 64, 64]         886,272         886,272\n",
      "       DownSample-6                            [32, 256, 64, 64]               0               0\n",
      "       DoubleConv-7                            [32, 256, 32, 32]       3,542,016       3,542,016\n",
      "       DownSample-8                            [32, 512, 32, 32]               0               0\n",
      "       DoubleConv-9                            [32, 512, 16, 16]      14,161,920      14,161,920\n",
      "        UpSample-10                           [32, 1024, 16, 16]       2,097,664       2,097,664\n",
      "   CropAndConcat-11         [32, 512, 32, 32], [32, 512, 32, 32]               0               0\n",
      "      DoubleConv-12                           [32, 1024, 32, 32]       7,080,960       7,080,960\n",
      "        UpSample-13                            [32, 512, 32, 32]         524,544         524,544\n",
      "   CropAndConcat-14         [32, 256, 64, 64], [32, 256, 64, 64]               0               0\n",
      "      DoubleConv-15                            [32, 512, 64, 64]       1,771,008       1,771,008\n",
      "        UpSample-16                            [32, 256, 64, 64]         131,200         131,200\n",
      "   CropAndConcat-17     [32, 128, 128, 128], [32, 128, 128, 128]               0               0\n",
      "      DoubleConv-18                          [32, 256, 128, 128]         443,136         443,136\n",
      "        UpSample-19                          [32, 128, 128, 128]          32,832          32,832\n",
      "   CropAndConcat-20       [32, 64, 256, 256], [32, 64, 256, 256]               0               0\n",
      "      DoubleConv-21                          [32, 128, 256, 256]         110,976         110,976\n",
      "          Conv2d-22                           [32, 64, 256, 256]              65              65\n",
      "=================================================================================================\n",
      "Total params: 31,043,521\n",
      "Trainable params: 31,043,521\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================= Hierarchical Summary =================================================\n",
      "\n",
      "Unet(\n",
      "  (down_conv): ModuleList(\n",
      "    (0): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 1,792 params\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 36,928 params\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 38,976 params\n",
      "    ), 38,976 params\n",
      "    (1): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 73,856 params\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 147,584 params\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 221,952 params\n",
      "    ), 221,952 params\n",
      "    (2): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 295,168 params\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 590,080 params\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 886,272 params\n",
      "    ), 886,272 params\n",
      "    (3): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 1,180,160 params\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 1,024 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 2,359,808 params\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 1,024 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 3,542,016 params\n",
      "    ), 3,542,016 params\n",
      "  ), 4,689,216 params\n",
      "  (down_sample): ModuleList(\n",
      "    (0): DownSample(\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 0 params\n",
      "    (1): DownSample(\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 0 params\n",
      "    (2): DownSample(\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 0 params\n",
      "    (3): DownSample(\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), 0 params\n",
      "    ), 0 params\n",
      "  ), 0 params\n",
      "  (middle_conv): DoubleConv(\n",
      "    (dconv): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 4,719,616 params\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 2,048 params\n",
      "      (2): ReLU(), 0 params\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 9,438,208 params\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 2,048 params\n",
      "      (5): ReLU(), 0 params\n",
      "    ), 14,161,920 params\n",
      "  ), 14,161,920 params\n",
      "  (up_sample): ModuleList(\n",
      "    (0): UpSample(\n",
      "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2)), 2,097,664 params\n",
      "    ), 2,097,664 params\n",
      "    (1): UpSample(\n",
      "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2)), 524,544 params\n",
      "    ), 524,544 params\n",
      "    (2): UpSample(\n",
      "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2)), 131,200 params\n",
      "    ), 131,200 params\n",
      "    (3): UpSample(\n",
      "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2)), 32,832 params\n",
      "    ), 32,832 params\n",
      "  ), 2,786,240 params\n",
      "  (up_conv): ModuleList(\n",
      "    (0): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 4,719,104 params\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 1,024 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 2,359,808 params\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 1,024 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 7,080,960 params\n",
      "    ), 7,080,960 params\n",
      "    (1): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 1,179,904 params\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 590,080 params\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 512 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 1,771,008 params\n",
      "    ), 1,771,008 params\n",
      "    (2): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 295,040 params\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 147,584 params\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 256 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 443,136 params\n",
      "    ), 443,136 params\n",
      "    (3): DoubleConv(\n",
      "      (dconv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 73,792 params\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "        (2): ReLU(), 0 params\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, padding_mode=reflect), 36,928 params\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 128 params\n",
      "        (5): ReLU(), 0 params\n",
      "      ), 110,976 params\n",
      "    ), 110,976 params\n",
      "  ), 9,406,080 params\n",
      "  (concat): ModuleList(\n",
      "    (0): CropAndConcat(), 0 params\n",
      "    (1): CropAndConcat(), 0 params\n",
      "    (2): CropAndConcat(), 0 params\n",
      "    (3): CropAndConcat(), 0 params\n",
      "  ), 0 params\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1)), 65 params\n",
      "), 31,043,521 params\n",
      "\n",
      "\n",
      "========================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pms.summary(model, torch.zeros((32, 3, 256, 256)), show_input=True, show_hierarchical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
